{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Open Ecosystem Challenges","text":"<p>Welcome to Open Ecosystem Challenges! \ud83d\ude80</p> <p>These are hands-on, recurring prompts designed to help you practice Cloud Native, OpenTelemetry, AI/ML, and other open source skills.</p> <p>Each challenge runs in a pre-provisioned environment, so you can focus on solving real problems, not setup headaches.</p> <p>What makes these challenges special:</p> <ul> <li>\ud83c\udfaf Skill-focused - Target specific technologies with clear objectives</li> <li>\ud83d\udcd6 Story-driven - Learn through engaging narratives</li> <li>\ud83d\ude80 Zero setup - Run in GitHub Codespaces, pre-configured and ready</li> <li>\u2705 Two-step verification - Smoke tests and GitHub Actions validate your solution</li> <li>\ud83c\udf93 Three levels - Beginner, Intermediate, and Expert for each adventure</li> </ul>"},{"location":"#available-adventures","title":"\ud83d\uddfa\ufe0f Available Adventures","text":"<p>Browse the available adventures and pick one that interests you:</p>"},{"location":"#february-2026-the-ai-observatory","title":"February 2026: The AI Observatory","text":"<p>Story: Investigate a mysterious bandwidth anomaly at a remote research station by instrumenting its AI system with OpenTelemetry.</p> Level Name \ud83e\udde0 Key Learnings \ud83d\udfe2 Beginner Calibrating the Lens <ul><li>Instrument Python AI apps with OpenLLMetry</li><li>Analyze traces in Jaeger</li></ul> \ud83d\udfe1 Intermediate The Distracted Pilot <ul><li>Instrument RAG pipelines with OpenLLMetry</li><li>Create custom OpenTelemetry metrics in Python</li><li>Write PromQL queries &amp; recording rules in Prometheus</li></ul> \ud83d\udd34 Expert The Noise Filter <ul><li>OpenTelemetry GenAI semantic conventions</li><li>Tail sampling in the OTel Collector</li></ul>"},{"location":"#january-2026-building-cloudhaven","title":"January 2026: Building CloudHaven","text":"<p>Story: Join the Infrastructure Guild and modernize CloudHaven's infrastructure from manual provisioning to a self-service platform using Infrastructure as Code.</p> Level Name \ud83e\udde0 Key Learnings \ud83d\udfe2 Beginner The Foundation Stones <ul><li>Infrastructure as Code with OpenTofu</li><li>Remote state management with GCS backend</li><li>Dynamic &amp; conditional resources</li></ul> \ud83d\udfe1 Intermediate The Modular Metropolis <ul><li>OpenTofu module testing with <code>tofu test</code></li><li>Test-Driven Development (TDD) workflow</li><li>Input validation with regex</li></ul> \ud83d\udd34 Expert The Guardian Protocols <ul><li>GitHub Actions for drift detection and plan/apply</li><li>Integration tests with service containers</li><li>Security scanning with Trivy</li></ul>"},{"location":"#december-2025-echoes-lost-in-orbit","title":"December 2025: Echoes Lost in Orbit","text":"<p>Story: Restore interstellar communications by fixing broken GitOps setups, progressive delivery systems, and observability pipelines across three galactic missions.</p> Level Name \ud83e\udde0 Key Learnings \ud83d\udfe2 Beginner Broken Echoes <ul><li>Debug GitOps flows with Argo CD</li><li>ApplicationSet templating &amp; pitfalls</li><li>Environment isolation &amp; namespaces</li><li>Sync policies: automated, prune &amp; self-heal</li></ul> \ud83d\udfe1 Intermediate The Silent Canary <ul><li>Progressive delivery with Argo Rollouts</li><li>Canary deployments &amp; automated analysis</li><li>Write PromQL queries for health validation</li><li>Kube-state-metrics for deployment decisions</li></ul> \ud83d\udd34 Expert Hyperspace Operations &amp; Transport <ul><li>Configure OpenTelemetry Collector pipelines</li><li>Spanmetrics connector (traces \u2192 metrics)</li><li>Detect \"idle canaries\" with traffic validation</li><li>Distributed tracing with Jaeger</li><li>Trace-derived metrics for progressive delivery</li></ul> <p>More adventures coming soon!</p>"},{"location":"#how-it-works","title":"\ud83c\udfae How It Works","text":"<p>Each level is independent - start anywhere, complete in any order. Levels share a connected story but have their own:</p> <ul> <li>Codespace configuration</li> <li>Documentation and guides</li> <li>Validation tests</li> </ul> <p>Levels:</p> <ul> <li>\ud83d\udfe2 Beginner: New to the technology? Start here to learn the basics</li> <li>\ud83d\udfe1 Intermediate: Comfortable with fundamentals? Practice advanced patterns</li> <li>\ud83d\udd34 Expert: Want a real challenge? Tackle complex real-world scenarios</li> </ul>"},{"location":"#how-to-verify-your-solution","title":"\u2705 How to Verify Your Solution","text":"<p>Each challenge includes a two-step verification process:</p> <ol> <li>Smoke Test - Run locally in your Codespace for quick validation</li> <li>GitHub Actions Workflow - Comprehensive verification you manually trigger after pushing</li> </ol> <p>\ud83d\udcd6 Learn more: Read the complete Verification Guide for detailed instructions on both steps.</p>"},{"location":"#faq","title":"\u2753 FAQ","text":"<p>Do I need to complete levels in order? No! Each level is independent. Start wherever you feel comfortable.</p> <p>Can I use these for team training? Absolutely! Perfect for upskilling, onboarding, internal training, and hackathons.</p> <p>Are there costs? GitHub Codespaces offers free hours per month - usually sufficient for individual use. Check GitHub's pricing for details.</p> <p>Need help? Check adventure-specific docs, open an issue, or start a discussion.</p>"},{"location":"#ready-to-start","title":"\ud83d\ude80 Ready to Start?","text":"<p>Choose your adventure and begin learning!</p>"},{"location":"start-a-challenge/","title":"Start a Challenge","text":"<p>The setup process is the same for all challenges. Fork the Open Ecosystem Challenges repository, start a Codespace with your challenge's configuration, and wait for the infrastructure to deploy.</p>"},{"location":"start-a-challenge/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Click the Fork button in the top-right corner of the GitHub repo or use this link.</p>"},{"location":"start-a-challenge/#already-have-a-fork","title":"Already Have a Fork?","text":"<p>If you've completed a previous challenge and already have a fork, sync it first to get the latest updates:</p> <ol> <li>Go to your fork on GitHub</li> <li>Click Sync fork (above the file list)</li> <li>Click Update branch if changes are available</li> </ol> <p>This ensures you have the latest challenge content before starting a new level.</p>"},{"location":"start-a-challenge/#2-start-a-codespace","title":"2. Start a Codespace","text":"<ul> <li>From your fork, click the green Code button \u2192 Codespaces hamburger menu \u2192 New with options </li> <li>Select the configuration that matches your challenge (e.g., \"Adventure 01 | \ud83d\udfe2 Beginner (Broken Echoes)\" for the   beginner level of adventure 1)   </li> </ul> <p>\u26a0\ufe0f Important: The challenge will not work if you choose a configuration that does not match your challenge (or the default).</p>"},{"location":"start-a-challenge/#3-wait-for-infrastructure-to-deploy","title":"3. Wait for Infrastructure to Deploy","text":"<p>Your Codespace will automatically provision all necessary challenge infrastructure. This usually takes around 5-10 minutes.</p> <p>\ud83d\udca1 Tip: To check the progress press <code>Cmd + Shift + P</code> (or <code>Ctrl + Shift + P</code> on Windows/Linux) and search for <code>View Creation Log</code> (available after a few moments once the Codespace has initialized).</p> <p>Once complete, return to your specific challenge documentation for level-specific instructions on solving the challenge.</p>"},{"location":"verification/","title":"Verify Your Solution","text":"<p>Each challenge includes a three-step verification process to help you validate and share your solution:</p> <ol> <li>Local Smoke Test - Quick validation in your Codespace</li> <li>Full Verification Workflow - Comprehensive validation via GitHub Actions</li> <li>Submit Your Results - Share your success with the community</li> </ol> <p>This process is designed to give you confidence in your solution without directly revealing the answers.</p>"},{"location":"verification/#step-1-local-smoke-test","title":"\ud83e\uddea Step 1: Local Smoke Test","text":"<p>The smoke test is a script that runs directly in your Codespace to check basic success criteria.</p>"},{"location":"verification/#what-it-checks","title":"What It Checks","text":"<ul> <li>\u2705 Basic functionality (e.g., services are reachable)</li> <li>\u2705 Key resources are deployed correctly</li> <li>\u2705 Essential configuration is in place</li> </ul>"},{"location":"verification/#what-it-doesnt-check","title":"What It Doesn't Check","text":"<p>The smoke test deliberately avoids checking certain criteria to prevent revealing the solution. These more complex validations are performed by the full verification workflow.</p>"},{"location":"verification/#how-to-run","title":"How to Run","text":"<p>Each challenge level has its own smoke test script. Run it from the repository root:</p> <pre><code>adventures/&lt;adventure-name&gt;/&lt;level&gt;/smoke-test.sh\n</code></pre> <p>Example for Adventure 01, Beginner level:</p> <pre><code>adventures/01-echoes-lost-in-orbit/easy/smoke-test.sh\n</code></pre>"},{"location":"verification/#understanding-the-results","title":"Understanding the Results","text":"<p>\u2705 If the smoke test passes:</p> <ul> <li>Your solution likely meets all requirements</li> <li>Proceed to Step 2 for full verification</li> </ul> <p>\u274c If the smoke test fails:</p> <ul> <li>Review the error messages and hints provided</li> <li>Check your solution against the challenge objectives</li> <li>Make adjustments and run the test again</li> </ul>"},{"location":"verification/#step-2-full-verification-workflow","title":"\ud83d\udd04 Step 2: Full Verification Workflow","text":"<p>The full verification workflow runs on GitHub Actions and performs comprehensive validation of your solution.</p>"},{"location":"verification/#prerequisites","title":"Prerequisites","text":""},{"location":"verification/#enable-github-actions-in-your-fork","title":"Enable GitHub Actions in Your Fork","text":"<p>If this is a new fork, GitHub Actions workflows are disabled by default. You need to enable them:</p> <ol> <li>Go to your fork on GitHub</li> <li>Click the Actions tab</li> <li>Click the green button \"I understand my workflows, go ahead and enable them\"</li> </ol> <p>\ud83d\udca1 Note: This is a one-time setup for your fork. Once enabled, workflows will be available for all challenges.</p>"},{"location":"verification/#running-the-verification-workflow","title":"Running the Verification Workflow","text":"<p>The verification workflow validates more complex success criteria that cannot be checked locally without revealing the solution.</p>"},{"location":"verification/#when-to-run","title":"When to Run","text":"<p>Run the verification workflow after your smoke test passes.</p>"},{"location":"verification/#how-to-run_1","title":"How to Run","text":"<ol> <li> <p>Commit and push your changes to the <code>main</code> branch of your fork:    <pre><code>git add .\ngit commit -m \"Solved Adventure 01 - Easy level\"\ngit push origin main\n</code></pre></p> </li> <li> <p>Manually trigger the workflow on GitHub:</p> <ul> <li>Go to your fork on GitHub</li> <li>Click the Actions tab</li> <li>Select the \"Verify Adventure\" workflow from the left sidebar</li> <li>Click the \"Run workflow\" dropdown button</li> <li>Select the challenge you want to verify (e.g., <code>Adventure 01 | \ud83d\udfe2 Easy (Broken Echoes)</code>)</li> <li>Click \"Run workflow\"</li> </ul> </li> <li> <p>Wait for the workflow to complete</p> </li> </ol>"},{"location":"verification/#understanding-the-results_1","title":"Understanding the Results","text":"<p>\u2705 If the workflow passes:</p> <ul> <li>\ud83c\udf89 Congratulations! You've successfully completed the challenge!</li> <li>Proceed to Step 3 to claim your completion</li> </ul> <p>\u274c If the workflow fails:</p> <ul> <li>Click on the failed workflow run to see detailed logs</li> <li>Review what criteria were not met</li> <li>Adjust your solution and try again</li> <li>Don't hesitate to open a discussion if you're stuck</li> </ul>"},{"location":"verification/#step-3-submit-your-results","title":"\ud83d\udcf8 Step 3: Submit Your Results","text":"<p>Once your verification workflow passes, it's time to share your success with the community!</p>"},{"location":"verification/#how-to-submit","title":"How to Submit","text":"<ol> <li> <p>Take a screenshot of your successful workflow run on GitHub Actions</p> <ul> <li>The screenshot should show the green checkmark and \"Success\" status</li> <li>Include the workflow name and your GitHub username in the screenshot</li> </ul> </li> <li> <p>Post your screenshot as a comment to the original challenge thread</p> <ul> <li>Find the discussion thread for your specific adventure and level</li> <li>Add a comment with your screenshot</li> <li>Optionally, share any interesting learnings or challenges you faced \ud83d\ude4c</li> </ul> </li> <li> <p>Celebrate! \ud83c\udf89</p> <ul> <li>You've officially completed the challenge</li> <li>Your contribution is now part of the Open Ecosystem community</li> <li>Ready for more? Move on to the next level or choose another adventure!</li> </ul> </li> </ol>"},{"location":"verification/#why-submit","title":"Why Submit?","text":"<ul> <li>Recognition: Get acknowledged by the community for your achievement</li> <li>Inspiration: Help motivate others who are working on the same challenge</li> <li>Community: Connect with fellow learners and share insights</li> <li>Progress Tracking: Keep a record of your completed challenges</li> </ul>"},{"location":"verification/#tips-for-success","title":"\ud83c\udfaf Tips for Success","text":"<ul> <li>Read the challenge objectives carefully: They outline exactly what needs to be achieved</li> <li>Run the smoke test before committing: It provides fast feedback during development</li> <li>Check the workflow logs: They contain valuable debugging information if verification fails</li> <li>Don't give up: These challenges are designed to be... challenging! Learning happens through iteration</li> </ul>"},{"location":"verification/#need-help","title":"\ud83e\udd1d Need Help?","text":"<p>If you're stuck or have questions about verification:</p> <ul> <li>\ud83d\udcac Start a discussion</li> <li>\ud83d\udc1b Report an issue if you think something is   broken</li> <li>\ud83d\udcd6 Check the adventure-specific documentation for hints and resources</li> </ul>"},{"location":"verification/#why-this-verification-process","title":"\ud83d\udd12 Why This Verification Process?","text":"<p>The three-step approach balances learning, validation, and community engagement:</p> <ul> <li>Smoke tests give you fast, local feedback without an internet connection</li> <li>Workflow verification ensures comprehensive validation without giving away solutions in the local scripts</li> <li>Community submission celebrates your achievement and contributes to the learning ecosystem </li> </ul> <p>Together, they provide confidence that your solution is correct while preserving the learning experience</p> <p>Happy solving! \ud83d\ude80</p>"},{"location":"01-echoes-lost-in-orbit/","title":"\ud83d\udef0\ufe0f Adventure 01: Echoes Lost in Orbit","text":"<p>Welcome to the first challenge in the Open Ecosystem Challenge series! Your mission: restore interstellar communication by fixing a broken GitOps setup. This is a hands-on troubleshooting exercise using Kubernetes, Argo CD, and Kustomize.</p> <p>The entire infrastructure is pre-provisioned in your Codespace \u2014 Kubernetes cluster, Argo CD, and sample app are ready to go. You don\u2019t need to set up anything locally. Just focus on solving the problem.</p>"},{"location":"01-echoes-lost-in-orbit/#the-backstory","title":"\ud83e\ude90 The Backstory","text":"<p>Welcome aboard the GitOps Starliner, a multi-species engineering vessel orbiting the vibrant planet of Polaris-9. Life in this quadrant is wonderfully diverse \u2014 from the whispering cloud-dwellers of Nebulon to the rhythmic click-speakers of Crustacea Prime.</p> <p>Communication between species used to be seamless, thanks to the Echo Server, a universal translator that instantly echoed your words in the listener's native format.</p> <p>But lately, something's off.</p> <p>Messages are getting scrambled. Some transmissions never arrive. The Echo Server, deployed across the Staging Moonbase and the Production Outpost, is no longer syncing properly. The Argo CD dashboard shows no active deployments, and telemetry is suspiciously quiet.</p> <p>You've been assigned to restore interstellar communication before the next critical mission.</p>"},{"location":"01-echoes-lost-in-orbit/#choose-your-level","title":"\ud83c\udfae Choose Your Level","text":"<p>Each level is a standalone challenge with its own Codespace that builds on the story while being technically independent \u2014 pick your level and start wherever you feel comfortable!</p> <p>\ud83d\udca1 Not sure which level to choose? Learn more about levels</p>"},{"location":"01-echoes-lost-in-orbit/#beginner-broken-echoes","title":"\ud83d\udfe2 Beginner: Broken Echoes","text":"<p>Status: \u2705 Available Topics: ArgoCD ApplicationSets, GitOps fundamentals</p> <p>The Echo Server is misbehaving. Both environments seem to be down, and messages are silent. Your mission: investigate the ArgoCD configuration and restore proper multi-environment delivery.</p> <p>Start the Beginner Challenge</p>"},{"location":"01-echoes-lost-in-orbit/#intermediate-the-silent-canary","title":"\ud83d\udfe1 Intermediate: The Silent Canary","text":"<p>Status: \u2705 Available Topics: Argo Rollouts, Progressive Delivery, Prometheus</p> <p>After fixing the communication outage, the Intergalactic Union welcomed a new species: the Zephyrians. The communications team attempted to deploy their language files using a progressive delivery system, but the rollout is failing. Your mission: debug the broken canary deployment and bring the Zephyrians' voices online.</p> <p>Start the Intermediate Challenge</p>"},{"location":"01-echoes-lost-in-orbit/#expert-echoes-in-the-dark","title":"\ud83d\udd34 Expert: Echoes in the Dark","text":"<p>Status: \u2705 Available Topics: Argo Rollouts, Progressive Delivery, Prometheus, Open Telemetry, Jaeger</p> <p>After fixing the Zephyrian communications, word of your progressive release mastery spread across the galaxy. The Bytari, a highly advanced species from the Andromeda sector, were impressed. \ud83c\udf1f</p> <p>They want to apply progressive delivery to their mission-critical service: HotROD (Hyperspace Operations &amp; Transport - Rapid Orbital Dispatch), an interstellar ride-sharing service handling dispatch requests across thousands of star systems. Every millisecond of latency matters, and any error could strand travelers between dimensions.</p> <p>Start the Expert Challenge</p>"},{"location":"01-echoes-lost-in-orbit/beginner/","title":"\ud83d\udfe2 Beginner: Broken Echoes","text":"<p>The Echo Server is misbehaving. Both environments seem to be down, and messages are silent. Your mission: investigate the ArgoCD configuration and restore proper multi-environment delivery.</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 10 December 2025 at 09:00 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#solution-walkthrough","title":"\ud83d\udcdd Solution Walkthrough","text":"<p>\u26a0\ufe0f Spoiler Alert: The following walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p> <p>Need help restoring multi-environment delivery? Follow the step-by-step beginner solution walkthrough to learn how to:</p> <ul> <li>Investigate the Argo CD ApplicationSet and spot common pitfalls</li> <li>Adjust the Argo CD ApplicationSet to meet the challenge objective</li> <li>Understand the reasoning behind each change, not just the commands</li> </ul> <p>The guide is written to explain not just what to do, but why. Dive in and level up your GitOps skills!</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should:</p> <ul> <li>See two distinct Applications in the Argo CD dashboard (one per environment)</li> <li>Ensure each Application deploys to its own isolated namespace</li> <li>Make the system resilient so Argo CD automatically reverts manual changes made to the cluster</li> <li>Confirm that updates happen automatically without leaving stale resources behind</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How Argo CD ApplicationSets work</li> <li>How to reason about templating and sync policies</li> <li>How drift detection and self-healing operate in GitOps workflows</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with   the cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"01-echoes-lost-in-orbit/beginner/#1-fork-the-repository","title":"1. Fork the Repository","text":"<ul> <li>Click the \"Fork\" button in the top-right corner of the GitHub repo or   use this link.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#2-start-a-codespace","title":"2. Start a Codespace","text":"<ul> <li>From your fork, click the green Code button \u2192 Codespaces hamburger menu \u2192 New with options.   </li> <li>Select the Adventure 01 | \ud83d\udfe2 Beginner (Broken Echoes) configuration.   </li> </ul> <p>\u26a0\ufe0f Important: The challenge will not work if you choose another configuration (or the default).</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#3-wait-for-infrastructure-to-deploy","title":"3. Wait for Infrastructure to Deploy","text":"<ul> <li>Your Codespace will automatically provision a Kubernetes cluster, Argo CD, and the sample app. This usually takes   around 5 minutes.</li> </ul> <p>\ud83d\udca1 Tip: To check the progress press <code>Cmd + Shift + P</code> (or <code>Ctrl + Shift + P</code> on Windows/Linux) and search for <code>View Creation Log</code> (available after a few moments once the Codespace has initialized).</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#4-access-the-argo-cd-dashboard","title":"4. Access the Argo CD Dashboard","text":"<ul> <li>Open the Ports tab in the bottom panel</li> <li>Find the Argo CD row (port <code>30100</code>) and click the forwarded address</li> </ul> <ul> <li>Log in using:   <pre><code>Username: readonly\nPassword: a-super-secure-password\n</code></pre></li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#5-fix-the-configuration","title":"5. Fix the Configuration","text":"<ul> <li>All errors are located in this ApplicationSet:   <pre><code>adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre></li> <li>Learn more about ApplicationSets and   the Application Specification in   the ArgoCD docs.</li> </ul> <p>\ud83d\udce6 About Kustomize: This challenge uses Kustomize under the hood to manage Kubernetes manifests. Kustomize allows us to maintain a base set of manifests (deployment, service) and apply environment-specific customizations through overlays (staging, prod). Each overlay can modify the base configuration\u2014like changing replica counts or namespaces\u2014without duplicating YAML. Argo CD automatically detects and applies these Kustomize configurations, so you don't need to run Kustomize commands manually. Your focus is on fixing the ApplicationSet to properly reference these Kustomize-managed paths.</p> <ul> <li>After making changes, apply them:   <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre>   (Run from the repo root)</li> </ul>"},{"location":"01-echoes-lost-in-orbit/beginner/#6-verify-your-solution","title":"6. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>adventures/01-echoes-lost-in-orbit/beginner/smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/beginner/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with    the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"01-echoes-lost-in-orbit/expert/","title":"\ud83d\udd34 Expert: Hyperspace Operations &amp; Transport","text":"<p>After fixing the Zephyrian communications, word of your progressive release mastery spread across the galaxy. The Bytari, a highly advanced species from the Andromeda sector, were impressed. \ud83c\udf1f</p> <p>They want to apply progressive delivery to their mission-critical service: HotROD (Hyperspace Operations &amp; Transport - Rapid Orbital Dispatch), an interstellar ride-sharing service handling dispatch requests across thousands of star systems. Every millisecond of latency matters, and any error could strand travelers between dimensions.</p> <p>Here's the catch: a previous engineer started instrumenting HotROD with OpenTelemetry and configured Argo Rollouts for automated validation, but left the setup incomplete. The observability pipeline is broken.</p> <p>The Bytari don't use staging/production environments\u2014they believe in single-environment progressive delivery validated purely by trace-derived metrics and automated health checks.</p> <p>Your mission: Fix the observability pipeline and canary validation. Make HotROD deployment-ready with proper distributed tracing.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 14 January 2026 at 09:00 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#solution-walkthrough","title":"\ud83d\udcdd Solution Walkthrough","text":"<p>\u26a0\ufe0f Spoiler Alert: The following walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p> <p>Need help deploying HotROD? Follow the step-by-step solution walkthrough to level up your progressive delivery skills!</p>"},{"location":"01-echoes-lost-in-orbit/expert/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should have:</p> <ul> <li>Automated rollout progression to HotROD version <code>1.76.0</code> driven by observability signals</li> <li>OpenTelemetry Collector configured with:<ul> <li>OTLP receiver for traces from HotROD</li> <li>Spanmetrics connector converting traces as metrics</li> <li>Trace export to Jaeger, metrics export to Prometheus</li> </ul> </li> <li>Canary analysis validating deployments with 3 queries:<ul> <li>Traffic detection ensuring minimum request rate (&gt;= 0.05 req/s) to the canary to prevent \"idle canaries\" that get promoted but never had real traffic. You can use the <code>hotrod_requests_total</code> metric to verify this</li> <li>Error rate thresholds (&lt; 5%)</li> <li>Latency thresholds for the 95th percentile (&lt; 1000ms)</li> </ul> </li> </ul> <p>The Bytari engineer who started this setup left an architecture diagram that should help you getting started:</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/expert/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>Configure OpenTelemetry Collector pipelines (receivers, connectors, exporters)</li> <li>Use the Span Metrics Connector to convert traces into metrics</li> <li>Prevent \"idle canaries\" that deploy successfully but were never really tested because they received no traffic</li> <li>Integrate distributed tracing for automated rollout decisions</li> <li>Write PromQL queries based on app and trace-derived metrics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with the   cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> <li>Argo CD CLI: Manage Argo CD applications from   the command line</li> <li>Argo Rollouts kubectl plugin: Extended   kubectl commands for managing Argo rollouts</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"01-echoes-lost-in-orbit/expert/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"Adventure 01 | \ud83d\udd34 Expert (Hyperspace Operations &amp; Transport)\"</li> <li>Wait ~5-10 minutes for all infrastructure to deploy (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul> <p>\u26a0\ufe0f After the infrastructure deploys, the setup script automatically starts port forwarding to the Argo Rollouts dashboard. This keeps the terminal busy, which is expected behavior. Your environment is fully ready when you see the terminal output shown below. Just open a new terminal to run commands.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/expert/#2-access-the-uis","title":"2. Access the UIs","text":"<ul> <li>Open the Ports tab in the bottom panel to access the following UIs</li> </ul> <p>\ud83d\udca1 Not a fan of user interfaces? No problem, you can also use the CLI tools to complete the challenge. But if you're new(ish) to these tools, the UIs can help you get familiar faster.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#argo-cd-port-30100","title":"Argo CD (Port 30100)","text":"<p>The Argo CD UI shows the sync status of your applications and allows you to refresh them after pushing new commits.</p> <ul> <li>Find the Argo CD row (port 30100) and click the forwarded address</li> <li>Log in using:   <pre><code>Username: readonly\nPassword: a-super-secure-password\n</code></pre></li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#argo-rollouts-port-30101","title":"Argo Rollouts (Port 30101)","text":"<p>The Argo Rollouts dashboard shows canary deployment progress and analysis status.</p> <ul> <li>Find the Argo Rollouts row (port 30101) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#prometheus-port-30102","title":"Prometheus (Port 30102)","text":"<p>The Prometheus UI helps you explore available metrics and test your PromQL queries.</p> <ul> <li>Find the Prometheus row (port 30102) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#jaeger-port-30103","title":"Jaeger (Port 30103)","text":"<p>The Jaeger UI shows distributed traces from HotROD. You can use it to verify that tracing is working end-to-end.</p> <ul> <li>Find the Jaeger row (port 30103) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#3-fix-the-configuration","title":"3. Fix the Configuration","text":"<p>The Bytari are counting on you. The HotROD service is deployed but the observability pipeline is broken, preventing new releases. Your task is to investigate, identify, and fix the issues.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like. The architecture diagram above shows how the components should connect. Use the Argo Rollouts dashboard, Prometheus UI, and Jaeger UI to debug and validate your changes.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#where-to-look","title":"Where to Look","text":"<p>All manifests are located in:</p> <pre><code>adventures/01-echoes-lost-in-orbit/expert/manifests/\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/expert/#deploy-your-changes","title":"Deploy Your Changes","text":"<p>After making your fixes, commit and push them to trigger the deployment:</p> <pre><code>git add adventures/01-echoes-lost-in-orbit/expert/manifests/\ngit commit -m \"Fix configuration\"\ngit push\n</code></pre> <p>\ud83d\udca1 Tip: If you're pushing to a branch other than <code>main</code>, make sure to also update the <code>ApplicationSet</code> in <code>adventures/01-echoes-lost-in-orbit/expert/manifests/appset.yaml</code> to point to your branch.</p> <p>Argo CD will automatically sync your changes after some time. You can speed things up by refreshing the applications manually. Depending on what you changed, use one of the following commands:</p> <pre><code>argocd app get hotrod --refresh\nargocd app get otel --refresh\n</code></pre> <p>If you made changes to HotROD, trigger a new rollout after ArgoCD synced your changes:</p> <pre><code>kubectl argo rollouts retry rollout hotrod -n hotrod\n</code></pre> <p>If you made changes to the Open Telemetry Collector Config, make sure to restart the collector for them to take effect:</p> <pre><code>kubectl rollout restart daemonset/collector -n otel\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/expert/#monitor-the-rollout","title":"Monitor the Rollout","text":"<p>Watch the canary deployment progress in the Argo Rollouts dashboard or use the CLI:</p> <pre><code>kubectl argo rollouts get rollout hotrod -n hotrod --watch\n</code></pre> <p>The rollout should automatically progress through the canary stages based on the analysis metrics validation.</p>"},{"location":"01-echoes-lost-in-orbit/expert/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>OpenTelemetry Collector Configuration</li> <li>Span Metrics Connector</li> <li>Argo Rollouts Analysis</li> <li>PromQL Basics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/expert/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"01-echoes-lost-in-orbit/expert/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>adventures/01-echoes-lost-in-orbit/expert/smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/expert/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/","title":"\ud83d\udfe1 Intermediate: The Silent Canary","text":"<p>After fixing the communication outage in Level 1, the Intergalactic Union welcomed a new species: the Zephyrians \ud83c\udf1f</p> <p>The communications team attempted to deploy their language files using a progressive delivery system, but the rollout is failing. The Zephyrians are still waiting to communicate with the rest of the galaxy.</p> <p>A previous engineer configured automated canary deployments with health checks but left the setup incomplete. Your mission: debug the broken rollout and bring the Zephyrians' voices online.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 24 December 2025 at 09:00 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#solution-walkthrough","title":"\ud83d\udcdd Solution Walkthrough","text":"<p>\u26a0\ufe0f Spoiler Alert: The following walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p> <p>Need help onboarding the Zephyrian language pack? Follow the step-by-step solution walkthrough to level up your progressive delivery skills!</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should have:</p> <ul> <li>Pod info version 6.9.3 deployed successfully in both staging and production environments</li> <li>Rollouts automatically progress through canary stages based on health metrics</li> <li>Two working PromQL queries in the <code>AnalysisTemplate</code> that validate application health during releases</li> <li>All rollouts complete successfully</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How to write PromQL queries to monitor application health</li> <li>How progressive delivery reduces deployment risk with automated validation</li> <li>How to debug and fix broken canary deployments</li> <li>How Argo Rollouts and Prometheus work together to   make data-driven deployment decisions</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with the   cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> <li>Argo CD CLI: Manage Argo CD applications from   the command line</li> <li>Argo Rollouts kubectl plugin: Extended   kubectl commands for managing Argo rollouts</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"01-echoes-lost-in-orbit/intermediate/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"Adventure 01 | \ud83d\udfe1 Intermediate (The Silent Canary)\"</li> <li>Wait ~5-10 minutes for all infrastructure to deploy (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul> <p>\u26a0\ufe0f After the infrastructure deploys, the setup script automatically starts port forwarding to the Argo Rollouts dashboard. This keeps the terminal busy, which is expected behavior. Your environment is fully ready when you see the terminal output shown below. Just open a new terminal to run commands.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#2-access-the-uis","title":"2. Access the UIs","text":"<ul> <li>Open the Ports tab in the bottom panel to access the following UIs   </li> </ul> <p>\ud83d\udca1 Not a fan of user interfaces? No problem, you can also use the CLI tools to complete the challenge. But if you're new(ish) to these tools, the UIs can help you get familiar faster.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#argo-cd-port-30100","title":"Argo CD (Port 30100)","text":"<p>The Argo CD UI shows the sync status of your applications and allows you to refresh them after pushing new commits.</p> <ul> <li>Find the Argo CD row (port 30100) and click the forwarded address</li> <li>Log in using:   <pre><code>Username: readonly\nPassword: a-super-secure-password\n</code></pre></li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#argo-rollouts-port-30101","title":"Argo Rollouts (Port 30101)","text":"<p>The Argo Rollouts dashboard shows canary deployment progress and analysis status.</p> <ul> <li>Find the Argo Rollouts row (port 30101) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#prometheus-port-30102","title":"Prometheus (Port 30102)","text":"<p>The Prometheus UI helps you explore available metrics and test your PromQL queries.</p> <ul> <li>Find the Prometheus row (port 30102) and click the forwarded address</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#3-fix-the-configuration","title":"3. Fix the Configuration","text":"<p>The Zephyrians are waiting for their language pack, but there are misconfigurations preventing the rollout from completing successfully. Your task is to investigate, identify, and fix the issues.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like. The Argo Rollouts dashboard and Prometheus UI can help you debug and validate your changes.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#where-to-look","title":"Where to Look","text":"<p>All manifests are located in:</p> <pre><code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/\n</code></pre> <p>\ud83d\udce6 About Kustomize: This challenge uses Kustomize under the hood to manage Kubernetes manifests. Kustomize allows us to maintain a base set of manifests and apply environment-specific customizations through overlays (staging, prod). Each overlay can modify the base configuration\u2014like changing replica counts or namespaces\u2014without duplicating YAML. With the <code>ApplicationSet</code>, Argo CD automatically detects and applies these Kustomize configurations, so you don't need to run Kustomize commands manually.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#deploy-your-changes","title":"Deploy Your Changes","text":"<p>After making your fixes, commit and push them to trigger the deployment:</p> <pre><code>git add adventures/01-echoes-lost-in-orbit/intermediate/manifests/\ngit commit -m \"Fix configuration\"\ngit push\n</code></pre> <p>\ud83d\udca1 Tip: If you're pushing to a branch other than <code>main</code>, make sure to also update the <code>ApplicationSet</code> in <code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/appset.yaml</code> to point to your branch.</p> <p>Argo CD will automatically sync your changes after some time. You can speed things up by refreshing the applications manually:</p> <pre><code>argocd app get echo-server-staging --refresh\nargocd app get echo-server-prod --refresh\n</code></pre> <p>After ArgoCD syncs your changes, trigger the rollout:</p> <pre><code>kubectl argo rollouts retry rollout echo-server -n echo-staging\nkubectl argo rollouts retry rollout echo-server -n echo-prod\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/intermediate/#monitor-the-rollout","title":"Monitor the Rollout","text":"<p>Watch the canary deployment progress in the Argo Rollouts dashboard or use the CLI:</p> <pre><code>kubectl argo rollouts get rollout echo-server -n echo-staging --watch\nkubectl argo rollouts get rollout echo-server -n echo-prod --watch\n</code></pre> <p>The rollout should automatically progress through the canary stages (33% \u2192 66% \u2192 100%).</p> <p>\u2139\ufe0f Note: In real-world progressive delivery, updates are typically deployed to staging first, validated, and then promoted to production. For this challenge, both environments update simultaneously to simplify the workflow and focus on learning canary rollouts and health checks.</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>Argo Rollouts</li> <li>Analysis and Progressive Delivery</li> <li>PromQL Basics</li> <li>Kubernetes Metrics exported by   kube-state-metrics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/intermediate/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>adventures/01-echoes-lost-in-orbit/intermediate/smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/intermediate/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/","title":"\ud83d\udfe2 Beginner Solution: Broken Echoes","text":"<p>Congratulations on taking on the first Open Ecosystem Challenge! In this walkthrough, we'll approach the challenge exactly as you would: start with the objectives, break them down one by one, and systematically fix what's broken. \ud83d\ude80</p> <p>\u26a0\ufe0f Spoiler Alert: This walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#step-1-review-the-challenge-objectives","title":"\ud83d\udccb Step 1: Review the Challenge Objectives","text":"<p>Let's start by reading what we need to achieve. The challenge states:</p> <p>By the end of this level, you should:</p> <ul> <li>See two distinct Applications in the Argo CD dashboard (one per environment)</li> <li>Ensure each Application deploys to its own isolated namespace</li> <li>Make the system resilient so Argo CD automatically reverts manual changes made to the cluster</li> <li>Confirm that updates happen automatically without leaving stale resources behind</li> </ul> <p>Perfect! Now we have four clear objectives to work toward. Let's tackle them one by one.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#step-2-understand-the-setup","title":"\ud83d\udd0d Step 2: Understand the Setup","text":"<p>Before we start fixing things, let's understand what we're working with. The challenge uses:</p> <ul> <li>Argo CD ApplicationSet: Automatically generates Applications for multiple environments</li> <li>Git directory generator: Scans for directories in the <code>overlays/</code> folder (finds <code>staging</code> and <code>prod</code>)</li> <li>Kustomize: Manages environment-specific configurations<ul> <li>Base: Common configuration (deployment, service)</li> <li>Overlays: Environment-specific customizations (staging and prod)</li> </ul> </li> </ul> <p>The file we need to fix is:</p> <pre><code>adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Let's open it:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: echo-server\n  namespace: argocd\nspec:\n  generators:\n    - git:\n        repoURL: __REPO_URL__\n        revision: HEAD\n        directories:\n          - path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/*\n  template:\n    metadata:\n      name: echo-server\n      labels:\n        app.kubernetes.io/managed-by: argocd\n    spec:\n      project: default\n      source:\n        repoURL: __REPO_URL__\n        targetRevision: HEAD\n        path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/{{path.basename}}\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: echo\n      syncPolicy:\n        syncOptions:\n          - CreateNamespace=true\n</code></pre> <p>Now let's work through each objective.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#step-3-clear-objectives","title":"\ud83c\udfaf Step 3: Clear Objectives","text":""},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-1-see-two-distinct-applications-in-the-argo-cd-dashboard","title":"Objective 1: See Two Distinct Applications in the Argo CD Dashboard","text":"<p>When opening the Argo CD dashboard (port <code>30100</code>), we won\u2019t see any applications.</p> <p></p> <p>Let's check if the ApplicationSet is present:</p> <pre><code>kubectl get applicationset -n argocd\n</code></pre> <p>You should see <code>echo-server</code> listed. That means Argo CD is at least aware of our ApplicationSet.</p> <p>Let's dig deeper and check the status of the ApplicationSet:</p> <pre><code>kubectl get applicationset echo-server -n argocd -o yaml\n</code></pre> <p>Scroll down to the <code>status.conditions</code> section. You should see an error like:</p> <pre><code>- message: 'ApplicationSet echo-server contains applications with duplicate name: echo-server'\n  reason: ErrorOccurred\n  status: \"False\"\n</code></pre> <p>This error means there are duplicate application names. Let's look at how the name is set in the template.</p> <p>The manifest shows:</p> <pre><code>metadata:\n  name: echo-server\n  namespace: argocd\n</code></pre> <p>For each environment (staging and prod), Argo CD tries to create an application called <code>echo-server</code>. This fails because Kubernetes resources must have unique names in a namespace, but the ApplicationSet template uses a static name for every generated Application. The Git directory generator creates one Application per overlay directory, so you need a unique name for each.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#how-to-fix","title":"How to fix?","text":"<p>Let's update the name field to use a template variable:</p> <pre><code>name: echo-server-{{path.basename}}\n</code></pre> <p>This way, Argo CD will generate <code>echo-server-staging</code> and <code>echo-server-prod</code>. One for each environment. The <code>{{path.basename}}</code> variable is replaced with the directory name (e.g., <code>staging</code> or <code>prod</code>) for each overlay, ensuring uniqueness.</p> <p>Now, let's apply the fix:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Go back to the dashboard. You should now see two progressing applications.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ApplicationSets use templates to generate multiple Applications, and names must be unique per environment.</li> <li>The Git directory generator creates one Application per overlay directory, so template variables like <code>{{path.basename}}</code> are essential for dynamic naming.</li> <li>The ApplicationSet and Application status are helpful for troubleshooting errors.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading","title":"Further Reading","text":"<ul> <li>Argo CD ApplicationSet Documentation</li> <li>ApplicationSet Templates</li> <li>Git Directory Generator</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-2-ensure-each-application-deploys-to-its-own-isolated-namespace","title":"Objective 2: Ensure each Application deploys to its own isolated namespace","text":"<p>We now have two applications showing up in Argo CD. But they are both deploying to the same namespace: <code>echo</code>.</p> <p>Each environment should at least have its own namespace for isolation and to avoid resource conflicts. This can be achieved by using a template variable in the namespace field, just like with the application name.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#how-to-fix_1","title":"How to fix?","text":"<p>Update the namespace field in the ApplicationSet manifest to include the <code>{{path.basename}}</code> variable:</p> <pre><code>spec.template.spec.destination.namespace: echo-{{path.basename}}\n</code></pre> <p>Now, let's apply the fix:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>This will set the destination namespace to <code>echo-staging</code> and <code>echo-prod</code> for the respective environments.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>How to use template variables for namespace isolation in multi-environment setups.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading_1","title":"Further Reading","text":"<ul> <li>Kubernetes Namespaces</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-3-make-the-system-resilient-so-argo-cd-automatically-reverts-manual-changes-made-to-the-cluster","title":"Objective 3: Make the system resilient so Argo CD automatically reverts manual changes made to the cluster","text":"<p>Our Applications are now deploying to separate namespaces, but they are not syncing yet and users can still make manual changes to the cluster. Let's split this objective into two clear steps:</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#fix-1-enable-auto-sync","title":"Fix 1: Enable auto sync","text":"<p>By default, Argo CD does not automatically sync Applications. This means changes in Git are not applied to the cluster unless you manually trigger a sync (which is disabled for this challenge). To fix this, let's enable auto sync in the ApplicationSet manifest:</p> <pre><code>syncPolicy:\n  automated:\n    enabled: true\n  syncOptions:\n    - CreateNamespace=true\n</code></pre> <p>Apply the change:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Now, Argo CD will automatically sync the Applications whenever there are changes in Git.</p> <p></p> <p>\ud83d\udca1 By default, this takes up to 3 minutes to detect changes. can speed this up by configuring a webhook to ArgoCD but for this challenge manually refreshing is just fine.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#fix-2-enable-self-healing-automatic-reverts","title":"Fix 2: Enable self-healing (automatic reverts)","text":"<p>Auto sync alone will not revert manual changes made directly in the cluster. To make Argo CD truly resilient, let's enable self-healing. This will ensure that any drift from the desired state in Git is automatically corrected.</p> <p>Update the ApplicationSet manifest to add <code>selfHeal: true</code>:</p> <pre><code>syncPolicy:\n  automated:\n    enabled: true\n    selfHeal: true\n  syncOptions:\n    - CreateNamespace=true\n</code></pre> <p>Apply the change:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Let's test self-healing by making a manual change:</p> <pre><code>kubectl scale deployment echo-server-staging -n echo-staging --replicas=3\nkubectl get pods -n echo-staging -w\n</code></pre> <p>Within seconds, Argo CD will detect the drift and scale it back down to 1 replica (as defined in Git).</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways_2","title":"Key Takeaways","text":"<ul> <li>Auto sync ensures that changes in Git are automatically applied to the cluster.</li> <li>Self-healing ensures the cluster matches the desired state in Git, even after manual changes.</li> <li>Auto sync and self-heal together enforce GitOps and keep your cluster consistent.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading_2","title":"Further Reading","text":"<ul> <li>Argo CD Automated Sync Policy</li> <li>Self-Healing in Argo CD</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#objective-4-confirm-that-updates-happen-automatically-without-leaving-stale-resources-behind","title":"Objective 4: Confirm that updates happen automatically without leaving stale resources behind","text":"<p>Now that our applications can sync automatically and self-heal, let's make sure that deleting resources from Git also removes them from the cluster. Otherwise, we risk leaving behind stale resources that are no longer needed.</p> <p>By default, Argo CD does not remove resources from the cluster when they are deleted from Git. Pruning must be explicitly enabled to keep the cluster clean and in sync with Git.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#how-to-fix_2","title":"How to fix?","text":"<p>Let's enable pruning by adding <code>prune: true</code> to the automated sync policy in the ApplicationSet manifest:</p> <pre><code>syncPolicy:\n  automated:\n    enabled: true\n    selfHeal: true\n    prune: true\n  syncOptions:\n    - CreateNamespace=true\n</code></pre> <p>Apply the change:</p> <pre><code>kubectl apply -n argocd -f adventures/01-echoes-lost-in-orbit/beginner/manifests/appset.yaml\n</code></pre> <p>Now, Argo CD will automatically delete resources from the cluster when they're removed from Git.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#key-takeaways_3","title":"Key Takeaways","text":"<ul> <li>Pruning (in combination with self-heal) ensures the cluster matches Git exactly.</li> <li>Automated sync policies with pruning keep environments clean and up to date.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#further-reading_3","title":"Further Reading","text":"<ul> <li>Argo CD Pruning</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/beginner/#complete-solution","title":"\u2705 Complete Solution","text":"<p>Here's what your corrected ApplicationSet should look like with all fixes applied:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: echo-server\n  namespace: argocd\nspec:\n  generators:\n    - git:\n        repoURL: __REPO_URL__\n        revision: HEAD\n        directories:\n          - path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/*\n  template:\n    metadata:\n      name: echo-server-{{path.basename}}  # \u2705 Unique name per environment\n      labels:\n        app.kubernetes.io/managed-by: argocd\n    spec:\n      project: default\n      source:\n        repoURL: __REPO_URL__\n        targetRevision: HEAD\n        path: adventures/01-echoes-lost-in-orbit/beginner/manifests/overlays/{{path.basename}}\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: echo-{{path.basename}}  # \u2705 Environment-specific namespace\n      syncPolicy:\n        automated: \n          enabled: true     # \u2705 Automatic sync enabled\n          selfHeal: true    # \u2705 Reverts manual changes\n          prune: true       # \u2705 Deletes removed resources\n        syncOptions:\n          - CreateNamespace=true\n</code></pre> <p>That's it! Your ApplicationSet is now ready for resilient, automated, and clean multi-environment deployments with Argo CD.</p> <p>If you want to go further, use this setup to experiment with ArgoCD, or explore the intermediate challenge next.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/","title":"\ud83d\udd34 Expert Solution Walkthrough: Hyperspace Operations &amp; Transport","text":"<p>In this walkthrough, we'll approach the challenge exactly as you would: start with the objectives, break them down one by one, and systematically fix what's broken. \ud83d\ude80</p> <p>\u26a0\ufe0f Spoiler Alert: This walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#step-1-review-the-challenge-objectives","title":"\ud83d\udccb Step 1: Review the Challenge Objectives","text":"<p>Let's start by reading what we need to achieve. The challenge states:</p> <p>By the end of this level, you should have:</p> <ul> <li>Automated rollout progression to HotROD version 1.76.0 driven by observability signals</li> <li>OpenTelemetry Collector configured with:<ul> <li>OTLP receiver for traces from HotROD</li> <li>Spanmetrics connector converting traces as metrics</li> <li>Trace export to Jaeger, metrics export to Prometheus</li> </ul> </li> <li>Canary analysis validating deployments with 3 queries:<ul> <li>Traffic detection ensuring minimum request rate (&gt;= 0.05 req/s) to the canary to prevent idle canaries that get promoted but never had real traffic. You can use the hotrod_requests_total metric to verify this</li> <li>Error rate thresholds (&lt; 5%)</li> <li>Latency thresholds for the 95th percentile (&lt; 1000ms)</li> </ul> </li> </ul> <p>Perfect! Now we have three clear objectives to work toward. Let's tackle them one by one.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#step-2-understand-the-setup","title":"\ud83d\udd0d Step 2: Understand the Setup","text":"<p>Before we start fixing things, let's understand what we're working with. All files are located in the following path:</p> <pre><code>adventures/01-echoes-lost-in-orbit/expert/manifests\n</code></pre> <p>Let's see what it contains:</p> <ul> <li><code>appset.yaml</code>: An Argo CD ApplicationSet that deploys the 3 applications listed below using the Git directory generator</li> <li><code>hotrod/</code>: The configuration for the HotROD app<ul> <li><code>analysis-template.yaml</code>: An Argo Rollouts AnalysisTemplate that defines health checks during canary deployments</li> <li><code>rollout.yaml</code>: An Argo Rollouts Rollout resource that manages the canary deployment strategy</li> <li><code>service.yaml</code>: A Kubernetes Service that exposes the HotROD app</li> </ul> </li> <li><code>otel/</code>: The configuration for the OpenTelemetry Collector<ul> <li><code>config.yaml</code>: OpenTelemetry Collector configuration</li> <li><code>daemonset.yaml</code>: Kubernetes DaemonSet that runs the OpenTelemetry Collector</li> <li><code>service.yaml</code>: A Kubernetes Service that exposes the OpenTelemetry Collector</li> </ul> </li> <li><code>traffic-generator/</code>: A simple app that sends traffic to the HotROD service<ul> <li><code>deployment.yaml</code>: Kubernetes Deployment for the traffic generator</li> </ul> </li> </ul> <p>Now let's work through each objective.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#step-3-clear-objectives","title":"\ud83c\udfaf Step 3: Clear Objectives","text":""},{"location":"01-echoes-lost-in-orbit/solutions/expert/#objective-1-automated-rollout-progression-to-hotrod-version-1760-driven-by-observability-signals","title":"Objective 1: Automated rollout progression to HotROD version 1.76.0 driven by observability signals","text":"<p>This objective depends on fixing the other objectives first. Let's move on for now.</p> <p>But before we jump into the other objectives, let's open Argo CD and see what's there.</p> <p></p> <p>We can see 3 apps (matching those defined in the ApplicationSet):</p> <ul> <li><code>hotrod</code>: The HotROD app. This is broken because the last <code>AnalysisRun</code> failed. That's expected and therefore okay for now.</li> <li><code>otel</code>: The OpenTelemetry Collector. This app is progressing because the collector keeps crashing with the error <code>invalid configuration: no receiver configuration specified in config</code>. We need to fix the collector configuration to get this app working. Since this is part of Objective 2, let's move on for now.</li> <li><code>traffic-generator</code>: The traffic generator app. This app is healthy and running. Perfect!</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#objective-2-opentelemetry-collector-configured","title":"Objective 2: OpenTelemetry Collector configured","text":"<p>According to the objective, we need to configure 3 things:</p> <ul> <li>OTLP receiver for traces from HotROD</li> <li>Spanmetrics connector converting traces as metrics</li> <li>Trace export to Jaeger, metrics export to Prometheus</li> </ul> <p>Let's take a look at the <code>otel/config.yaml</code> file to find out what's already there and what's missing:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: collector-config\n  labels:\n    app: collector\ndata:\n  collector-config.yaml: |\n    receivers:\n\n    connectors:\n      spanmetrics:\n        dimensions:\n          - name: hotrod.namespace\n          - name: hotrod.pod.name\n          - name: hotrod.pod.hash\n\n    exporters:\n      debug:\n        verbosity: detailed\n      otlp:\n        endpoint: jaeger-collector.jaeger.svc.cluster.local:4317\n        tls:\n          insecure: true\n\n    extensions:\n      health_check:\n        endpoint: 0.0.0.0:13133\n\n    service:\n      extensions: [health_check]\n      pipelines:\n        traces:\n          receivers: [otlp]\n          exporters: [debug, otlp, spanmetrics]\n</code></pre> <p>An OpenTelemetry Collector config has several main sections. Here's what each of them does in a nutshell:</p> <ul> <li>Receivers: Define how the collector receives data (e.g., OTLP, Jaeger, Zipkin)</li> <li>Processors: Transform, filter, or enrich data before exporting (e.g., batch, memory limiter, attributes)</li> <li>Connectors: Connect two pipelines, acting as both exporter and receiver (e.g., spanmetrics converts traces to metrics)</li> <li>Exporters: Define where the collector sends the data (e.g., Jaeger, Prometheus, OTLP)</li> <li>Service Pipelines: Define the flow of data through receivers, processors, connectors, and exporters</li> </ul> <p>You can read more about this in the OpenTelemetry Collector documentation.</p> <p>OTLP receiver for traces from HotROD</p> <p>With that knowledge, the first thing we notice is that the <code>receivers</code> section is empty. That means the collector can't receive any data from HotROD. We need to add an OTLP receiver here.</p> <p>The question is: what configuration does it need? Does HotROD send traces via OTLP over gRPC or HTTP?</p> <p>Let's check the <code>rollout.yaml</code> file in the <code>hotrod/</code> folder to find out. There we can see:</p> <pre><code>env:\n  - name: OTEL_EXPORTER_OTLP_ENDPOINT\n    value: \"http://collector.otel.svc.cluster.local:4318\"\n</code></pre> <p>Port <code>4318</code> is the default port for OTLP/HTTP according to the OpenTelemetry Collector docs.</p> <p>Let's add an OTLP receiver configured for HTTP:</p> <pre><code>    receivers:\n      otlp:\n        protocols:\n          http:\n            endpoint: 0.0.0.0:4318\n</code></pre> <p>With that, the \"OTLP receiver for traces from HotROD\" is configured. Let's move on to the next one.</p> <p>Spanmetrics connector converting traces as metrics</p> <p>Looking at the <code>connectors</code> section, we can see that the <code>spanmetrics</code> connector is already defined. Spanmetrics traces are also already exported in the traces pipeline.</p> <p>Let's create a metrics pipeline to export the generated metrics. We can do that by adding a new <code>metrics</code> pipeline in the <code>service</code> section. For now, let's only export to debug. With that, the spanmetrics connector will convert traces into metrics and export them.</p> <p>Here's how the updated <code>service</code> section looks:</p> <pre><code>      pipelines:\n        traces:\n          ... # existing traces pipeline\n        metrics:\n          receivers: [ spanmetrics ]\n          exporters: [ debug ]\n</code></pre> <p>Trace export to Jaeger, metrics export to Prometheus</p> <p>Now that we have all the metrics and traces we need, we just need to export them to the right endpoints.</p> <p>Let's start with exporting traces to Jaeger. Checking the config, traces are already exported to an OTLP exporter which points to Jaeger. Perfect! That means trace export to Jaeger is already configured. Let's export metrics to Prometheus.</p> <p>For that, we need to add a Prometheus exporter in the <code>exporters</code> section and add it to the <code>metrics</code> pipeline. Let's check the OpenTelemetry Collector docs on how to configure the Prometheus exporter.</p> <p>According to the docs, we can add the following configuration:</p> <pre><code>    exporters:\n      ... # existing exporters\n      prometheus:\n        endpoint: \"1.2.3.4:1234\"\n</code></pre> <p>But what port should we use?</p> <p>Let's check the <code>daemonset.yaml</code> in the <code>otel/</code> folder. There we can see that the collector exposes port <code>8889</code> for Prometheus metrics (look for the port named <code>prometheus</code>). The pod annotations also confirm this with <code>prometheus.io/port: \"8889\"</code>. This means Prometheus is already configured to scrape metrics from this port.</p> <p>So the correct configuration is:</p> <pre><code>    exporters:\n      ... # existing exporters\n      prometheus:\n        endpoint: \"0.0.0.0:8889\"\n</code></pre> <p>We use <code>0.0.0.0</code> to listen on all interfaces so Prometheus can scrape the metrics from outside the container.</p> <p>Awesome! Now we just need to add the Prometheus exporter to the <code>metrics</code> pipeline:</p> <pre><code>      pipelines:\n        ... # existing pipelines\n        metrics:\n          receivers: [ spanmetrics ]\n          exporters: [ debug, prometheus ]\n</code></pre> <p>Let's push these changes, let Argo CD pick them up, restart the collector, and see if we can get the OpenTelemetry Collector working.</p> <pre><code>argocd app get otel --refresh\nkubectl rollout restart daemonset/collector -n otel\n</code></pre> <p>Nice! After pushing and letting Argo CD sync the changes, the OpenTelemetry Collector is now healthy. We can also see metrics arriving in Prometheus and traces in Jaeger. Objective 2 is complete!</p> <p>Jaeger:</p> <p></p> <p>Prometheus:</p> <p></p> <p>Here's the full OpenTelemetry Collector config:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: collector-config\n  labels:\n    app: collector\ndata:\n  collector-config.yaml: |\n    receivers:\n      otlp:\n        protocols:\n          http:\n            endpoint: 0.0.0.0:4318\n\n    connectors:\n      spanmetrics:\n        dimensions:\n          - name: hotrod.namespace\n          - name: hotrod.pod.name\n          - name: hotrod.pod.hash\n\n    exporters:\n      debug:\n        verbosity: detailed\n      otlp:\n        endpoint: jaeger-collector.jaeger.svc.cluster.local:4317\n        tls:\n          insecure: true\n      prometheus:\n        endpoint: \"0.0.0.0:8889\"\n\n    extensions:\n      health_check:\n        endpoint: 0.0.0.0:13133\n\n    service:\n      extensions: [health_check]\n      pipelines:\n        traces:\n          receivers: [otlp]\n          exporters: [debug, otlp, spanmetrics]\n        metrics:\n          receivers: [ spanmetrics ]\n          exporters: [ debug, prometheus ]\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>The OpenTelemetry Collector is highly configurable with receivers, processors/connectors, exporters, and service pipelines.</li> <li>The OTLP receiver allows the collector to receive traces from applications instrumented with OpenTelemetry.</li> <li>The Spanmetrics connector can convert traces into metrics, enabling advanced analysis and monitoring.</li> <li>Pipelines define the flow of data through the collector, allowing for flexible data processing and exporting.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#further-reading","title":"Further Reading","text":"<ul> <li>OpenTelemetry Collector Documentation</li> <li>Span Metrics Connector</li> <li>Prometheus Exporter for OpenTelemetry Collector</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#objective-3-canary-analysis-validating-deployments","title":"Objective 3: Canary analysis validating deployments","text":"<p>This objective tells us that we need to configure the AnalysisTemplate with 3 queries:</p> <ul> <li>Traffic detection ensuring minimum request rate (&gt;= 0.05 req/s) to the canary to prevent \"idle canaries\" that get promoted but never had real traffic. You can use the hotrod_requests_total metric to verify this.</li> <li>Error rate thresholds (&lt; 5%)</li> <li>Latency thresholds for the 95th percentile (&lt; 1000ms)</li> </ul> <p>Let's check the AnalysisTemplate in <code>hotrod/analysis-template.yaml</code> to see what's already there:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: hotrod-analysis\nspec:\n  args:\n    - name: namespace\n    - name: canary-hash\n  metrics:\n    - name: error-rate-lt-5-percent\n      initialDelay: 60s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 0.05\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # Error rate: ratio of failed requests to total requests\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\",\n              error=\"true\"\n            }[2m])) \n            /\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n\n    - name: latency-p95-lt-1s\n      initialDelay: 90s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 1000\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # P95 latency: should be under 1 second\n            histogram_quantile(0.95,\n              sum by (le) (rate(traces_span_metrics_duration_milliseconds_bucket{\n                hotrod_namespace=\"{{args.namespace}}\",\n                hotrod_pod_hash=\"{{args.canary-hash}}\"\n              }[2m]))\n            )\n</code></pre> <p>Let's also open the Argo Rollouts dashboard and see the status of the last analysis run. We can see that the first check <code>error-rate-lt-5-percent</code> succeeded, but the second check <code>latency-p95-lt-1s</code> failed with <code>index out of range</code>.</p> <p></p> <p>Let's take one step back and check and fix the queries one by one.</p> <p>Traffic detection (&gt;= 0.05 req/s)</p> <p>The existing queries check for error rate and latency, but there's no query for traffic detection. This metric ensures the canary receives real traffic before promotion\u2014without it, a canary could be promoted even if it never handled any requests.</p> <p>Let's add a new metric. The configuration is explained in the comments:</p> <pre><code>  metrics:\n    # This metric ensures the canary is receiving real traffic before promotion.\n    # Without this check, a canary could be promoted even if it never handled\n    # any requests (e.g., due to routing issues or no traffic at all).\n    - name: traffic-detection\n      initialDelay: 90s # Wait 90 seconds before the first check to allow traffic to flow\n      interval: 10s # Check every 10 seconds\n      count: 5 # Run 5 measurements total\n      successCondition: result[0] &gt;= 0.05 # Success if there are at least 0.05 req/s (ensures real traffic)\n      failureLimit: 3 # Allow up to 3 failures before marking the analysis as failed\n      inconclusiveLimit: 5 # Allow up to 5 inconclusive results (e.g., no data yet)\n      consecutiveErrorLimit: 3 # Allow up to 3 consecutive errors (e.g., Prometheus unavailable)\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          # Calculate the per-second request rate over the last 2 minutes\n          # for the canary pods only (filtered by namespace and pod hash)\n          query: |\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n</code></pre> <p>With that, we're checking if there is traffic and avoiding idle canaries. Let's move on to the next one.</p> <p>Error rate thresholds (&lt; 5%)</p> <p>A metric for this already exists:</p> <pre><code>    - name: error-rate-lt-5-percent\n      initialDelay: 60s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 0.05\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # Error rate: ratio of failed requests to total requests\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\",\n              error=\"true\"\n            }[2m])) \n            /\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n</code></pre> <p>This metric succeeded in the last analysis run. After checking it again to be sure, it looks good. No changes needed here.</p> <p>Latency thresholds for the 95th percentile (&lt; 1000ms)</p> <p>A metric for this already exists:</p> <pre><code>    - name: latency-p95-lt-1s\n      initialDelay: 90s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 1000\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # P95 latency: should be under 1 second\n            histogram_quantile(0.95,\n              sum by (le) (rate(traces_span_metrics_duration_milliseconds_bucket{\n                hotrod_namespace=\"{{args.namespace}}\",\n                hotrod_pod_hash=\"{{args.canary-hash}}\"\n              }[2m]))\n            )\n</code></pre> <p>This one failed in the last analysis run with this error:</p> <pre><code>Metric \"latency-p95-lt-1s\" assessed Error due to consecutiveErrors (3) &gt; consecutiveErrorLimit (2): \"Error Message: reflect: slice index out of range\"\n</code></pre> <p>But when looking at it, everything looks fine. Thinking about this again, this failed before we had the OpenTelemetry Collector working. That means no metrics about traces were arriving in Prometheus at all.</p> <p>Now that we have the OpenTelemetry Collector working, let's push our changes, refresh the Argo CD app, and retry the rollout to see if this works now.</p> <pre><code>argocd app get hotrod --refresh\nkubectl argo rollouts retry rollout hotrod -n hotrod\n</code></pre> <p>Nice! The analysis run succeeded and we can be sure that we're actually checking an app with real traffic now.</p> <p></p> <p>Here's the full <code>AnalysisTemplate</code> with all 3 metrics:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: hotrod-analysis\nspec:\n  args:\n    - name: namespace\n    - name: canary-hash\n  metrics:\n    - name: traffic-detection\n      initialDelay: 90s # Wait 90 seconds before the first check to allow traffic to flow\n      interval: 10s # Check every 10 seconds\n      count: 5 # Run 5 measurements total\n      successCondition: result[0] &gt;= 0.05 # Success if there are at least 0.05 req/s (ensures real traffic)\n      failureLimit: 3 # Allow up to 3 failures before marking the analysis as failed\n      inconclusiveLimit: 5 # Allow up to 5 inconclusive results (e.g., no data yet)\n      consecutiveErrorLimit: 3 # Allow up to 3 consecutive errors (e.g., Prometheus unavailable)\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          # Calculate the per-second request rate over the last 2 minutes\n          # for the canary pods only (filtered by namespace and pod hash)\n          query: |\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n    - name: error-rate-lt-5-percent\n      initialDelay: 60s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 0.05\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # Error rate: ratio of failed requests to total requests\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\",\n              error=\"true\"\n            }[2m])) \n            /\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n\n    - name: latency-p95-lt-1s\n      initialDelay: 90s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 1000\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # P95 latency: should be under 1 second\n            histogram_quantile(0.95,\n              sum by (le) (rate(traces_span_metrics_duration_milliseconds_bucket{\n                hotrod_namespace=\"{{args.namespace}}\",\n                hotrod_pod_hash=\"{{args.canary-hash}}\"\n              }[2m]))\n            )\n</code></pre>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>If there's no traffic, canary deployments can be promoted without ever being tested under real conditions.</li> <li>Argo Rollouts AnalysisTemplates can use traces for verification when converting them to metrics with the Spanmetrics connector.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#further-reading_1","title":"Further Reading","text":"<ul> <li>Argo Rollouts Analysis Documentation</li> <li>PromQL Basics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/expert/#complete-solution","title":"\u2705 Complete Solution","text":"<p>Here's what your corrected files should look like with all fixes applied:</p> <p>OpenTelemetry Collector Config: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: collector-config\n  labels:\n    app: collector\ndata:\n  collector-config.yaml: |\n    receivers:\n      otlp:\n        protocols:\n          http:\n            endpoint: 0.0.0.0:4318\n\n    connectors:\n      spanmetrics:\n        dimensions:\n          - name: hotrod.namespace\n          - name: hotrod.pod.name\n          - name: hotrod.pod.hash\n\n    exporters:\n      debug:\n        verbosity: detailed\n      otlp:\n        endpoint: jaeger-collector.jaeger.svc.cluster.local:4317\n        tls:\n          insecure: true\n      prometheus:\n        endpoint: \"0.0.0.0:8889\"\n\n    extensions:\n      health_check:\n        endpoint: 0.0.0.0:13133\n\n    service:\n      extensions: [health_check]\n      pipelines:\n        traces:\n          receivers: [otlp]\n          exporters: [debug, otlp, spanmetrics]\n        metrics:\n          receivers: [ spanmetrics ]\n          exporters: [ debug, prometheus ]\n</code></pre></p> <p>Analysis Template:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: hotrod-analysis\nspec:\n  args:\n    - name: namespace\n    - name: canary-hash\n  metrics:\n    - name: traffic-detection\n      initialDelay: 90s # Wait 90 seconds before the first check to allow traffic to flow\n      interval: 10s # Check every 10 seconds\n      count: 5 # Run 5 measurements total\n      successCondition: result[0] &gt;= 0.05 # Success if there are at least 0.05 req/s (ensures real traffic)\n      failureLimit: 3 # Allow up to 3 failures before marking the analysis as failed\n      inconclusiveLimit: 5 # Allow up to 5 inconclusive results (e.g., no data yet)\n      consecutiveErrorLimit: 3 # Allow up to 3 consecutive errors (e.g., Prometheus unavailable)\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          # Calculate the per-second request rate over the last 2 minutes\n          # for the canary pods only (filtered by namespace and pod hash)\n          query: |\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n    - name: error-rate-lt-5-percent\n      initialDelay: 60s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 0.05\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # Error rate: ratio of failed requests to total requests\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\",\n              error=\"true\"\n            }[2m])) \n            /\n            sum(rate(hotrod_requests_total{\n              namespace=\"{{args.namespace}}\",\n              rollouts_pod_template_hash=\"{{args.canary-hash}}\"\n            }[2m]))\n\n    - name: latency-p95-lt-1s\n      initialDelay: 90s\n      interval: 10s\n      count: 3\n      successCondition: result[0] &lt; 1000\n      failureLimit: 2\n      inconclusiveLimit: 3\n      consecutiveErrorLimit: 2\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # P95 latency: should be under 1 second\n            histogram_quantile(0.95,\n              sum by (le) (rate(traces_span_metrics_duration_milliseconds_bucket{\n                hotrod_namespace=\"{{args.namespace}}\",\n                hotrod_pod_hash=\"{{args.canary-hash}}\"\n              }[2m]))\n            )\n</code></pre> <p>With these changes, your rollouts will now progress automatically through canary stages based on health metrics. This also means Objective 1 is now achieved\u2014the rollout will automatically progress to HotROD version 1.76.0 driven by the observability signals we configured. All objectives of the challenge are met. Great job! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/","title":"\ud83d\udfe1 Intermediate Solution Walkthrough: The Silent Canary","text":"<p>In this walkthrough, we'll approach the challenge exactly as you would: start with the objectives, break them down one by one, and systematically fix what's broken. \ud83d\ude80</p> <p>\u26a0\ufe0f Spoiler Alert: This walkthrough contains the full solution to the challenge. We encourage you to try solving it on your own first. Consider coming back here only if you get stuck or want to check your approach.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#step-1-review-the-challenge-objectives","title":"\ud83d\udccb Step 1: Review the Challenge Objectives","text":"<p>Let's start by reading what we need to achieve. The challenge states:</p> <p>By the end of this level, you should have:</p> <ul> <li>Pod info version 6.9.3 deployed successfully in both staging and production environments</li> <li>Rollouts automatically progress through canary stages based on health metrics</li> <li>Two working PromQL queries in the AnalysisTemplate that validate application health during releases</li> <li>All rollouts complete successfully</li> </ul> <p>Perfect! Now we have four clear objectives to work toward. Let's tackle them one by one.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#step-2-understand-the-setup","title":"\ud83d\udd0d Step 2: Understand the Setup","text":"<p>Before we start fixing things, let's understand what we're working with. All files are located in the following path:</p> <pre><code>adventures/01-echoes-lost-in-orbit/intermediate/manifests\n</code></pre> <p>Let's see what it contains:</p> <ul> <li><code>appset.yaml</code>: An Argo CD ApplicationSet that generates Applications for staging and prod using the Git directory generator</li> <li><code>base/</code>: The base configuration for the echo-server app which is deployed using Kustomize<ul> <li><code>analysis-template.yaml</code>: An Argo Rollouts AnalysisTemplate that defines health checks during canary deployments</li> <li><code>rollout.yaml</code>: An Argo Rollouts Rollout resource that manages the canary deployment strategy</li> <li><code>service.yaml</code>: A Kubernetes Service that exposes the echo-server app</li> <li><code>kustomization.yaml</code>: Kustomize configuration file for the base</li> </ul> </li> <li><code>overlays/</code>: Environment-specific overlays for staging and prod<ul> <li><code>kustomization.yaml</code>: Kustomize configuration that adjust the number of replicas for each overlay</li> </ul> </li> </ul> <p>Now let's work through each objective.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#step-3-clear-objectives","title":"\ud83c\udfaf Step 3: Clear Objectives","text":"<p>Note: All steps in this guide use the staging environment. Since staging and production are identical (except for the number of replicas), you can follow the same steps for both. To keep things simple, we'll only mention staging throughout.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-1-pod-info-version-693-deployed-successfully-in-both-staging-and-production-environments","title":"Objective 1: Pod info version 6.9.3 deployed successfully in both staging and production environments","text":"<p>Let's start by checking which version of the podinfo image is currently running in staging:</p> <pre><code>kubectl -n echo-staging get rollout echo-server -o yaml\n</code></pre> <p>Let's look for the <code>spec.template.spec.containers[0].image</code> field. It should be set to <code>stefanprodan/podinfo:6.9.3</code>. Perfect, that's the version we want. However, if we scroll down to the <code>status</code> section, we'll notice an error:</p> <pre><code>  - message: 'Rollout aborted update to revision 2: Metric \"container-restarts\" assessed\n      Error due to consecutiveErrors (1) &gt; consecutiveErrorLimit (0): \"Error Message:\n      Post \"http://prom-server.prometheus.svc.cluster.local/api/v1/query\": dial tcp:\n      lookup prom-server.prometheus.svc.cluster.local on 10.96.0.10:53: no such host\"'\n    reason: RolloutAborted\n    status: \"False\"\n    type: Progressing\n</code></pre> <p>Let's find out which version is actually running. The rollout status includes a field called <code>status.stableRS</code>, which tells us the unique identifier of the replicaset that is currently considered stable (i.e., the one serving traffic). To understand what is actually running in our cluster, we can inspect this replicaset directly:</p> <pre><code># status.stableRS: 6fdd67656d\nkubectl -n echo-staging get replicaset echo-server-6fdd67656d -o yaml\n</code></pre> <p>Let's look for the spec.template.spec.containers[0].image field in the output. In this case, we see:</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        - name: echo-server\n          image: stefanprodan/podinfo:6.8.0\n</code></pre> <p>This tells us that the old image (<code>6.8.0</code>) is still running, even though we expect <code>6.9.3</code>. This means that the rollout is stuck and hasn't progressed to the new version.</p> <p>If we open the Argo Rollouts UI and select the <code>echo-staging</code> namespace in the top right corner, we'll see an error in the rollout\u2014this matches the output from the previous command.</p> <p></p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#how-to-fix","title":"How to fix?","text":"<p>We don't need to push image 6.9.3 because that has already been done. The problem is that the rollout is aborted due to an error. Let's move on to the next objective to investigate further.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Argo Rollouts won't progress a rollout if there are errors in the configuration.</li> <li>Check the rollout status and conditions for errors when rollouts don't progress.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#further-reading","title":"Further Reading","text":"<ul> <li>Progressive Delivery with Argo Rollouts</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-2-rollouts-automatically-progress-through-canary-stages-based-on-health-metrics","title":"Objective 2: Rollouts automatically progress through canary stages based on health metrics","text":"<p>As described in the previous objective, our rollout is currently not progressing due to an error. Let's investigate exactly what's going on.</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#how-to-fix_1","title":"How to fix?","text":"<p>First, let's take a closer look at the error message by running:</p> <pre><code>kubectl -n echo-staging get rollout echo-server -o yaml\n</code></pre> <p>In the output, we see this condition:</p> <pre><code>  - message: 'Rollout aborted update to revision 2: Metric \"container-restarts\" assessed\n      Error due to consecutiveErrors (1) &gt; consecutiveErrorLimit (0): \"Error Message:\n      Post \"http://prom-server.prometheus.svc.cluster.local/api/v1/query\": dial tcp:\n      lookup prom-server.prometheus.svc.cluster.local on 10.96.0.10:53: no such host\"'\n    reason: RolloutAborted\n    status: \"False\"\n    type: Progressing\n</code></pre> <p>This tells us that the <code>container-restarts</code> metric is failing because it can't reach the Prometheus server. This is likely due to an incorrect URL in our AnalysisTemplate.</p> <p>Let's check the AnalysisTemplate by running:</p> <pre><code>kubectl -n echo-staging get analysistemplate echo-analysis -o yaml\n</code></pre> <p>Or, we can simply look at the file in the repo at <code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/base/analysis-template.yaml</code>.</p> <p>We see that the spec contains two metrics: <code>container-restarts</code> and <code>ready-containers</code>.</p> <pre><code>spec:\n  args:\n  - name: namespace\n  metrics:\n  - consecutiveErrorLimit: 0\n    count: 1\n    failureLimit: 0\n    inconclusiveLimit: 0\n    name: container-restarts\n    provider:\n      prometheus:\n        address: http://prom-server.prometheus.svc.cluster.local\n        query: |\n          # There should be no restarts\n          sum(increase(kube_pod_container_status_restarts_total{\n            namespace=\"{{args.namespace}}\",\n            pod=~\"echo-server-.*\"\n          }[1m])) or vector(0)\n    successCondition: result[0] &gt; 0\n  - consecutiveErrorLimit: 0\n    count: 1\n    failureLimit: 0\n    inconclusiveLimit: 0\n    name: ready-containers\n    provider:\n      prometheus:\n        address: http://prometheus-server.prometheus.svc.cluster.local\n        query: |-\n          # Check how many containers are ready (should be at least 1)\n          # Look at kube_pod_container_status_* metrics\n          # Test in Prometheus UI (port 30102 in the \"Ports\" tab of VS Code)\n    successCondition: result[0] &gt;= 1\n</code></pre> <p>According to the error message, the problem is with the <code>container-restarts</code> metric: it can't reach the Prometheus server at <code>http://prom-server.prometheus.svc.cluster.local</code>.</p> <p>Let's check if this service exists. The URL follows the structure <code>http://&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>, so we run:</p> <pre><code>kubectl -n prometheus get service\n</code></pre> <p>This outputs one service called <code>prometheus-server</code>, not <code>prom-server</code>. It looks like there's a typo in our manifest. Let's fix it by changing <code>prom-server</code> to <code>prometheus-server</code> in <code>adventures/01-echoes-lost-in-orbit/intermediate/manifests/base/analysis-template.yaml</code>.</p> <p>After making the change, let's commit and push. For more details, see the challenge instructions.</p> <p>Now, let's retry the rollout:</p> <pre><code>kubectl argo rollouts retry rollout echo-server -n echo-staging\n</code></pre> <p>This time, instead of using plain <code>kubectl</code>, let's use the Argo Rollouts kubectl plugin for more detailed status:</p> <pre><code>kubectl argo rollouts -n echo-staging status echo-server\n</code></pre> <p>We see that the rollout is still not progressing:</p> <pre><code>Degraded - RolloutAborted: Rollout aborted update to revision 2: Metric \"container-restarts\" assessed Failed due to failed (1) &gt; failureLimit (0)\nError: The rollout is in a degraded state with message: RolloutAborted: Rollout aborted update to revision 2: Metric \"container-restarts\" assessed Failed due to failed (1) &gt; failureLimit (0)\n</code></pre> <p>At least now we have a new error! The <code>container-restarts</code> metric is failing because there was one failure, but the failure limit for the rollout to proceed is set to 0.</p> <p>Let's look at the current configuration:</p> <pre><code>    - name: container-restarts\n      successCondition: result[0] &gt; 0\n      failureLimit: 0\n      inconclusiveLimit: 0\n      consecutiveErrorLimit: 0\n      count: 1\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # There should be no restarts\n            sum(increase(kube_pod_container_status_restarts_total{\n              namespace=\"{{args.namespace}}\",\n              pod=~\"echo-server-.*\"\n            }[1m])) or vector(0)\n</code></pre> <p>The query checks for the number of container restarts in the last minute. If there are no restarts, the query returns 0. As the comment says, there should be no container restarts for success.</p> <p>But the success condition is set to <code>result[0] &gt; 0</code>, which means the metric only succeeds if there is at least one restart. That's the opposite of what we want! Let's change the success condition to <code>result[0] == 0</code>.</p> <p>After updating, commit and push your changes, then retry the rollout:</p> <pre><code>kubectl argo rollouts retry rollout echo-server -n echo-staging\n</code></pre> <p>This time, let's use the Argo Rollouts UI to check the status. Open the UI, select the <code>echo-staging</code> namespace in the top right, and click on the rollout card.</p> <p>After waiting a bit, we see the rollout still hasn't progressed and there are three analysis runs:</p> <p></p> <p>Clicking on <code>Analysis 2-1</code> shows the first error (wrong service), <code>Analysis 2-1.1</code> shows the previous error (wrong success condition), and <code>Analysis 2-1.2</code> is the latest error. Let's take a closer look:</p> <p></p> <p>Here, we see that the <code>container-restarts</code> metric was finally successful, but <code>ready-containers</code> is still failing. Let's click the metric on the left to investigate the query. It turns out it simply wasn't implemented yet:</p> <p></p> <p>Let's follow the comment and open the Prometheus UI to find a metric that helps us check the number of ready containers. By entering <code>kube_pod_container_status_</code>, autocompletion shows all available metrics. Let's choose <code>kube_pod_container_status_ready</code>.</p> <p></p> <p>Just like with the container-restarts metric, we want to filter for our namespace and pod. Let's add this and execute the query:</p> <pre><code>kube_pod_container_status_ready{\n  namespace=\"echo-staging\", \n  pod=~\"echo-server-.*\"\n}\n</code></pre> <p>This returns all ready containers for our pods:</p> <p></p> <p>But we don't want a list of all ready containers\u2014we want to check if there is at least one ready container. So, let's use the sum aggregator:</p> <pre><code>sum(\n  kube_pod_container_status_ready{\n    namespace=\"echo-staging\", \n    pod=~\"echo-server-.*\"\n  }\n)\n</code></pre> <p></p> <p>Nice! Now the query returns \"1\" instead of a list, which is exactly what we want.</p> <p>Before adding this to our manifest, let's remember we have two environments. Instead of hardcoding the namespace, let's use the same approach as in the other metric and use the <code>{{args.namespace}}</code> placeholder.</p> <p>The final query looks like this (with <code>or vector(0)</code> at the end to ensure the query returns 0 if no data is found):</p> <pre><code>query: |-\n  # Check how many containers are ready (should be at least 1)\n  sum(kube_pod_container_status_ready{\n    namespace=\"{{args.namespace}}\",\n    pod=~\"echo-server-.*\"\n  }) or vector(0)\n</code></pre> <p>Let's add this to our manifest, commit, push, refresh, and retry again.</p> <p>Yay! This time, the rollout actually progressed and the analysis runs are successful\u2014objective met! \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>There are multiple effective ways to debug Argo Rollouts. Try them and use your favorite.</li> <li>Service references use the format <code>svc.namespace.svc.cluster.local</code>.</li> <li>Prometheus queries are a simple and effective way to validate application health during rollouts.</li> <li>The Prometheus UI is a great way to test and build your queries.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#further-reading_1","title":"Further Reading","text":"<ul> <li>Argo Rollouts Analysis</li> <li>Prometheus Querying Basics</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-3-two-working-promql-queries-in-the-analysistemplate-that-validate-application-health-during-releases","title":"Objective 3: Two working PromQL queries in the AnalysisTemplate that validate application health during releases","text":"<p>This objective is closely tied to Objective 2. By fixing both metrics in the AnalysisTemplate (<code>container-restarts</code> and <code>ready-containers</code>) we ensured that application health is properly validated during each rollout. Both queries now work as intended:</p> <ul> <li><code>container-restarts</code>: Confirms there are no container restarts during the rollout.</li> <li><code>ready-containers</code>: Checks that at least one container is ready before progressing.</li> </ul> <p>With these working PromQL queries, our rollouts are now protected by robust health checks. \ud83c\udf89</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#key-takeaways_2","title":"Key Takeaways","text":"<ul> <li>PromQL queries in the AnalysisTemplate provide automated, reliable health validation for deployments.</li> </ul>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#objective-4-all-rollouts-complete-successfully","title":"Objective 4: All rollouts complete successfully","text":"<p>This objective is also directly connected to Objective 2 and 3. After correcting the metrics in the AnalysisTemplate, the rollout was able to progress through all canary stages and complete successfully in both environments.</p> <p>With all objectives met, our deployment process is now fully automated and resilient!</p>"},{"location":"01-echoes-lost-in-orbit/solutions/intermediate/#complete-solution","title":"\u2705 Complete Solution","text":"<p>Here's what your corrected <code>AnalysisTemplate</code> should look like with all fixes applied:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: echo-analysis\nspec:\n  args:\n    - name: namespace\n  metrics:\n    - name: container-restarts\n      successCondition: result[0] == 0\n      failureLimit: 0\n      inconclusiveLimit: 0\n      consecutiveErrorLimit: 0\n      count: 1\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |\n            # There should be no restarts\n            sum(increase(kube_pod_container_status_restarts_total{\n              namespace=\"{{args.namespace}}\",\n              pod=~\"echo-server-.*\"\n            }[1m])) or vector(0)\n    - name: ready-containers\n      successCondition: result[0] &gt;= 1\n      failureLimit: 0\n      inconclusiveLimit: 0\n      consecutiveErrorLimit: 0\n      count: 1\n      provider:\n        prometheus:\n          address: http://prometheus-server.prometheus.svc.cluster.local\n          query: |-\n            # Check how many containers are ready (should be at least 1)\n            sum(kube_pod_container_status_ready{\n              namespace=\"{{args.namespace}}\",\n              pod=~\"echo-server-.*\"\n            }) or vector(0)\n</code></pre> <p>With these changes, your rollouts will now progress automatically through canary stages based on health metrics, and all objectives of the challenge will be met. Great job! \ud83c\udf89</p>"},{"location":"02-building-cloudhaven/","title":"\ud83c\udf06\ufe0f Adventure 02: Building CloudHaven","text":"<p>Welcome to the second adventure in the Open Ecosystem Challenge series! Your mission: modernize CloudHaven's infrastructure from manual provisioning to a self-service platform. This is a hands-on journey through infrastructure as code with OpenTofu and GitHub Actions.</p> <p>The entire infrastructure is pre-provisioned in your Codespace \u2014 OpenTofu and mock cloud services are ready to go when you need them. You don't need to set up anything locally. Just focus on solving the problem.</p>"},{"location":"02-building-cloudhaven/#the-backstory","title":"\ud83c\udfd7\ufe0f The Backstory","text":"<p>Welcome to CloudHaven, a bustling digital metropolis where every district depends on essential services to thrive. You've just joined the Infrastructure Guild, a team of platform engineers responsible for providing the tools and services that keep the city running.</p> <p>CloudHaven is expanding rapidly. The Merchant's Quarter needs storage vaults for their goods and ledgers for tracking inventory. The Scholar's District requires secure archives for ancient texts. The Artisan's Quarter demands workshops with specialized tools. Each district has unique needs, but they all depend on the Guild to provide reliable, scalable infrastructure services.</p> <p>The Guild used to provision everything manually through cloud consoles \u2014 a process that was slow, error-prone, and impossible to track. Recently, they've started adopting Infrastructure as Code, but the transition is incomplete.</p> <p>The Guild Master has assigned you to complete the modernization journey.</p> <p>Your mission: Build the services and tools that will support CloudHaven's future growth.</p>"},{"location":"02-building-cloudhaven/#choose-your-level","title":"\ud83c\udfae Choose Your Level","text":"<p>Each level is a standalone challenge with its own Codespace that builds on the story while being technically independent \u2014 pick your level and start wherever you feel comfortable!</p> <p>\ud83d\udca1 Not sure which level to choose? Learn more about levels</p>"},{"location":"02-building-cloudhaven/#beginner-the-foundation-stones","title":"\ud83d\udfe2 Beginner: The Foundation Stones","text":"<p>Status: \u2705 Available Topics: OpenTofu, Remote State, Dynamic Resources</p> <p>The Guild needs essential services: storage vaults for merchant goods, ledger systems for tracking inventory, and eventually an audit archive to monitor trade across districts. A previous Guild engineer started provisioning these services using OpenTofu but left before finishing. The state is stored locally, making collaboration impossible, and some services remain half-configured or misconfigured.</p> <p>Your mission: Complete the OpenTofu configuration and establish proper state management.</p> <p>Start the Beginner Challenge</p>"},{"location":"02-building-cloudhaven/#intermediate-the-modular-metropolis","title":"\ud83d\udfe1 Intermediate: The Modular Metropolis","text":"<p>Status: \u2705 Available Topics: OpenTofu, Modules, Testing, Input Validation</p> <p>CloudHaven is thriving after you fixed the Foundation Stones! The city has grown to three districts, and the Guild  decided to refactor the infrastructure into reusable modules. A senior engineer started the work using Test-Driven  Development \u2014 writing tests first, then implementing. But they were called away before finishing, leaving behind  working tests... and buggy code that doesn't match them.</p> <p>Your mission: Fix the bugs, complete the integration test, and deploy the infrastructure.</p> <p>Start the Intermediate Challenge</p>"},{"location":"02-building-cloudhaven/#expert-the-guardian-protocols","title":"\ud83d\udd34 Expert: The Guardian Protocols","text":"<p>Status: \u2705 Available Topics: GitHub Actions, OpenTofu Plan/Apply, Security Scanning (Trivy)</p> <p>CloudHaven needs automated guardians: workflows that detect infrastructure drift, validate pull requests with plans, integration tests, and security scans, and then apply safe changes. A previous engineer started the setup but left it incomplete. Your mission: bring the Guardian Protocols online.</p> <p>Start the Expert Challenge</p>"},{"location":"02-building-cloudhaven/beginner/","title":"\ud83d\udfe2 Beginner: The Foundation Stones","text":"<p>The Merchant's Quarter needs essential services, but the previous Guild engineer left the OpenTofu configuration incomplete and misconfigured. Your mission: Fix the issues, complete the setup, and establish proper infrastructure management for the Guild.</p>"},{"location":"02-building-cloudhaven/beginner/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 4 February 2026 at 23:59 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"02-building-cloudhaven/beginner/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"02-building-cloudhaven/beginner/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should:</p> <ul> <li>Provision storage vaults and ledger databases for each district dynamically</li> <li>Deploy the audit database only when there is more than one district</li> <li>Store state remotely in a GCS backend following best practices so the Guild can collaborate</li> <li>Resolve all TODOs in the code and successfully run <code>tofu apply</code></li> </ul>"},{"location":"02-building-cloudhaven/beginner/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>OpenTofu/Terraform basics</li> <li>Remote state backends for team collaboration</li> <li>Dynamic resource provisioning with <code>for_each</code></li> <li>Conditional resources with the brand new <code>enabled</code> meta-argument</li> </ul>"},{"location":"02-building-cloudhaven/beginner/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>tofu</code>: The OpenTofu CLI for infrastructure provisioning</li> <li><code>gcp-api-mock</code>: A mock GCP API running locally to simulate cloud   resources without real cloud costs</li> </ul> <p>\u26a0\ufe0f Note: The mock API only supports Cloud Storage and Cloud SQL, and only the functions needed for this challenge have been properly tested.</p>"},{"location":"02-building-cloudhaven/beginner/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"02-building-cloudhaven/beginner/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"\ud83c\udf06 Adventure 02 | \ud83d\udfe2 Beginner (The Foundation Stones)\"</li> <li>Wait ~2 minutes for the environment to initialize (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul>"},{"location":"02-building-cloudhaven/beginner/#2-access-the-gcp-api-mock-ui","title":"2. Access the GCP API Mock UI","text":"<ul> <li>Open the Ports tab in the bottom panel</li> <li>Find the GCP API Mock row (port <code>30104</code>) and click the forwarded address</li> <li>This UI lets you explore the mock cloud resources (buckets, databases) created by your OpenTofu configuration</li> </ul>"},{"location":"02-building-cloudhaven/beginner/#3-fix-the-configuration","title":"3. Fix the Configuration","text":"<p>The Merchant's Quarter is waiting for their infrastructure, but the previous engineer left things in a mess. Your task is to investigate, identify, and fix the issues.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like. The GCP API Mock UI can help you explore the resources created by your configuration.</p>"},{"location":"02-building-cloudhaven/beginner/#where-to-look","title":"Where to Look","text":"<p>All OpenTofu files are located in:</p> <pre><code>adventures/02-building-cloudhaven/beginner/\n</code></pre> <p>\ud83d\udca1 Tip: Run <code>grep -r \"TODO\" .</code> to find all TODOs left by the previous engineer.</p> <p>Review the existing files and look for <code>TODO</code> comments:</p> <ul> <li><code>main.tf</code>: Provider and backend configuration</li> <li><code>state.tf</code>: State bucket configuration</li> <li><code>variables.tf</code>: District definitions</li> <li><code>merchants.tf</code>: Resource definitions for vaults and ledgers</li> <li><code>audit.tf</code>: Audit database configuration</li> <li><code>outputs.tf</code>: Infrastructure outputs</li> </ul>"},{"location":"02-building-cloudhaven/beginner/#apply-your-changes","title":"Apply Your Changes","text":"<p>After making your fixes, test and apply them using <code>tofu apply</code>.</p> <p>\ud83d\udca1 Tip: If you change the backend configuration, you'll need to run <code>tofu init -migrate-state</code> to migrate the state to the new backend.</p>"},{"location":"02-building-cloudhaven/beginner/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>OpenTofu Documentation</li> <li>OpenTofu Meta-Arguments</li> <li>OpenTofu Backend Configuration</li> <li>Google Cloud Provider</li> </ul>"},{"location":"02-building-cloudhaven/beginner/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"02-building-cloudhaven/beginner/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the repo root:</p> <pre><code>./smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"02-building-cloudhaven/beginner/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"02-building-cloudhaven/expert/","title":"\ud83d\udd34 Expert: The Guardian Protocols","text":"<p>After the Modular Metropolis refactoring, CloudHaven flourished. But with growth came risk. One night, a rogue change slipped through unnoticed and nearly brought down the North Market's trading vaults. The Council was furious \u2014 how could this happen without anyone noticing?</p> <p>The Guild Master summoned you urgently. \"We need guardians,\" she said, \"automated sentinels that watch over our infrastructure day and night. They must catch dangerous changes before they reach the city, detect when reality drifts from our blueprints, and sound the alarm when threats appear.\"</p> <p>A previous engineer began building these Guardian Protocols using GitHub Actions, but was reassigned before completing them. The workflows exist, but they're incomplete and broken. Your mission: bring the Guardian Protocols online and protect CloudHaven from chaos.</p>"},{"location":"02-building-cloudhaven/expert/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 4 February 2026 at 23:59 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"02-building-cloudhaven/expert/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"02-building-cloudhaven/expert/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, your workflows should:</p> <ul> <li>Detect infrastructure drift<ul> <li>Run <code>tofu plan</code> to check for changes</li> <li>Create a PR when drift is found</li> </ul> </li> <li>Validate pull requests<ul> <li>Run <code>tofu plan</code> and comment results on the PR</li> <li>Run tests against the mock GCP API</li> <li>Scan for security vulnerabilities and comment results on the PR</li> <li>Fail on critical or high severity vulnerabilities</li> </ul> </li> <li>Apply infrastructure automatically<ul> <li>Apply changes when a PR is merged to main</li> </ul> </li> </ul> <p>All three workflows must have succeeded at least once.</p>"},{"location":"02-building-cloudhaven/expert/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>GitHub Actions workflows for infrastructure automation</li> <li>Infrastructure drift detection with <code>tofu plan</code></li> <li>Security scanning with Trivy</li> <li>Running integration tests with service containers</li> <li>The TF-via-PR action for plan/apply workflows</li> </ul>"},{"location":"02-building-cloudhaven/expert/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>tofu</code>: The OpenTofu CLI for infrastructure provisioning</li> <li><code>gcp-api-mock</code>: A mock GCP API running locally to simulate cloud   resources without real cloud costs</li> <li>GitHub Actions: The workflows you'll be fixing are in <code>.github/workflows/</code></li> </ul> <p>\u26a0\ufe0f Note: The mock API only supports Cloud Storage and Cloud SQL, and only the functions needed for this challenge have been properly tested.</p>"},{"location":"02-building-cloudhaven/expert/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"02-building-cloudhaven/expert/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"\ud83c\udf06 Adventure 02 | \ud83d\udd34 Expert (The Guardian Protocols)\"</li> <li>Wait ~2 minutes for the environment to initialize (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul>"},{"location":"02-building-cloudhaven/expert/#2-access-the-gcp-api-mock-ui","title":"2. Access the GCP API Mock UI","text":"<ul> <li>Open the Ports tab in the bottom panel</li> <li>Find the GCP API Mock row (port <code>30104</code>) and click the forwarded address</li> <li>This UI lets you explore the mock cloud resources (buckets, databases) created by your OpenTofu configuration</li> </ul> <p>\u2139\ufe0f Note: You may see a warning when accessing the port. This is expected: the port is set to public so that GitHub Actions runners can access the mock API during workflow runs. Click \"Continue\" to proceed.</p> <p></p>"},{"location":"02-building-cloudhaven/expert/#3-fix-the-workflows","title":"3. Fix the Workflows","text":"<p>The previous engineer started building the Guardian Protocols but left before finishing. The workflows exist but are incomplete and broken.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like.</p> <p>\u2139\ufe0f Note: The OpenTofu configuration is correct. You don't need to modify any <code>.tf</code> files. Focus on fixing the GitHub Actions workflows.</p>"},{"location":"02-building-cloudhaven/expert/#how-the-workflows-should-work-together","title":"How the Workflows Should Work Together","text":"<p>Once fixed, the Guardian Protocols work like this:</p> <ul> <li> <p>Drift Detection: Trigger manually via the GitHub Actions UI. If drift is detected, a draft PR is created with    a drift log entry.</p> <p>\u2139\ufe0f The infrastructure has intentional drift. The workflow should detect changes and create a PR.</p> </li> <li> <p>Mark PR Ready for Review: The PR is created as a draft intentionally. You must click \"Ready for Review\" to    trigger the validation workflow. This is a    GitHub Actions limitation. </p> </li> <li> <p>Validation: Once the PR is ready for review, the validation workflow runs tests and security scans.</p> </li> <li> <p>Apply: When the PR is merged to main, the apply workflow automatically reconciles the infrastructure.</p> </li> </ul>"},{"location":"02-building-cloudhaven/expert/#where-to-look","title":"Where to Look","text":"<p>The workflows are located in:</p> <pre><code>.github/workflows/\n\u251c\u2500\u2500 adventure02-expert-detect-drift.yaml            # Drift detection workflow\n\u251c\u2500\u2500 adventure02-expert-validate-changes.yaml        # PR validation workflow\n\u2514\u2500\u2500 adventure02-expert-apply-infrastructure.yaml    # Apply workflow\n</code></pre>"},{"location":"02-building-cloudhaven/expert/#deploy-your-changes","title":"Deploy Your Changes","text":"<p>After making your fixes, commit and push them to main:</p> <pre><code>git add .github/workflows/\ngit commit -m \"Fix workflows\"\ngit push\n</code></pre> <p>To test your changes on an existing PR:</p> <ol> <li>Go to your PR</li> <li>Click \"Convert to draft\" (under \"Reviewers\" on the right)</li> <li>Click \"Ready for review\" again</li> </ol> <p>This re-triggers the validation workflow with your latest changes.</p> <p>\u2139\ufe0f Tip: Re-running a failed workflow uses the code from the original run. Use the draft toggle above to pick up new changes pushed to main.</p> <p>To trigger the drift detection workflow manually:</p> <ol> <li>Go to the Actions tab in your repository</li> <li>Select \"\ud83c\udf06\ud83d\udd34 | \ud83d\udd0d Detect Infrastructure Drift\"</li> <li>Click \"Run workflow\"</li> </ol>"},{"location":"02-building-cloudhaven/expert/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>GitHub Actions</li> <li>GitHub Actions Service Containers</li> <li>OpenTofu Plan Command</li> <li>Trivy Action</li> <li>TF-via-PR Action</li> </ul>"},{"location":"02-building-cloudhaven/expert/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"02-building-cloudhaven/expert/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the challenge directory:</p> <pre><code>cd adventures/02-building-cloudhaven/expert\n./smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"02-building-cloudhaven/expert/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"02-building-cloudhaven/intermediate/","title":"\ud83d\udfe1 Intermediate: The Modular Metropolis","text":"<p>After fixing the Foundation Stones, CloudHaven is thriving! The city has grown to three districts, and the Guild decided to refactor the infrastructure into reusable modules. A senior engineer started the work using Test-Driven Development - writing tests first, then implementing. But they were called away before finishing, leaving behind working tests... and buggy code that doesn't match them. Your mission: fix the bugs, complete the integration test, and deploy the infrastructure.</p>"},{"location":"02-building-cloudhaven/intermediate/#deadline","title":"\u23f0 Deadline","text":"<p>Wednesday, 4 February 2026 at 23:59 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"02-building-cloudhaven/intermediate/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"02-building-cloudhaven/intermediate/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should have:</p> <ul> <li>All tests of the districts module pass</li> <li>A completed integration test that applies infrastructure against the mock GCP API to verify end-to-end   functionality</li> <li>Three districts deployed with correctly configured infrastructure (vaults and ledgers)</li> </ul> <p>\u2139\ufe0f Important: The tests are correct - they define the expected behavior. Your job is to fix the implementation to match what the tests expect. Don't modify existing tests unless a comment tells you to; let the tests guide your fixes.</p>"},{"location":"02-building-cloudhaven/intermediate/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>OpenTofu module structure &amp; testing with <code>tofu test</code></li> <li>Test-Driven Development (TDD) workflow</li> <li>Input validation</li> <li>How to use the <code>moved</code> block for refactoring infrastructure</li> </ul>"},{"location":"02-building-cloudhaven/intermediate/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>tofu</code>: The OpenTofu CLI for infrastructure provisioning</li> <li><code>gcp-api-mock</code>: A mock GCP API running locally to simulate cloud   resources without real cloud costs</li> </ul> <p>\u26a0\ufe0f Note: The mock API only supports Cloud Storage and Cloud SQL, and only the functions needed for this challenge have been properly tested.</p>"},{"location":"02-building-cloudhaven/intermediate/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"02-building-cloudhaven/intermediate/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"\ud83c\udf06 Adventure 02 | \ud83d\udfe1 Intermediate (The Modular Metropolis)\"</li> <li>Wait ~2 minutes for the environment to initialize (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul>"},{"location":"02-building-cloudhaven/intermediate/#2-access-the-gcp-api-mock-ui","title":"2. Access the GCP API Mock UI","text":"<ul> <li>Open the Ports tab in the bottom panel</li> <li>Find the GCP API Mock row (port <code>30104</code>) and click the forwarded address</li> <li>This UI lets you explore the mock cloud resources (buckets, databases) created by your OpenTofu configuration</li> </ul>"},{"location":"02-building-cloudhaven/intermediate/#3-fix-the-configuration","title":"3. Fix the Configuration","text":"<p>The Guild's senior engineer started refactoring the infrastructure into modules but left before finishing. The tests are failing, and the configuration has bugs.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like.</p>"},{"location":"02-building-cloudhaven/intermediate/#where-to-look","title":"Where to Look","text":"<p>All OpenTofu files are located in:</p> <pre><code>adventures/02-building-cloudhaven/intermediate/\n\u251c\u2500\u2500 main.tf                    # Provider and backend configuration\n\u251c\u2500\u2500 variables.tf               # Input variables\n\u251c\u2500\u2500 districts.tf               # Module calls for each district\n\u251c\u2500\u2500 outputs.tf                 # Infrastructure outputs\n\u251c\u2500\u2500 moved.tf                   # Resource migration blocks\n\u251c\u2500\u2500 modules/district/          # The district module (fix bugs here)\n\u2502   \u251c\u2500\u2500 main.tf                # Locals and tier configuration\n\u2502   \u251c\u2500\u2500 variables.tf           # Input validation\n\u2502   \u251c\u2500\u2500 vault.tf               # Storage bucket resource\n\u2502   \u251c\u2500\u2500 ledger.tf              # Cloud SQL resource\n\u2502   \u251c\u2500\u2500 outputs.tf             # Module outputs\n\u2502   \u2514\u2500\u2500 tests/                 # Module tests (read these!)\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 integration.tftest.hcl # Complete this test\n</code></pre> <p>\ud83d\udca1 Tip: Run <code>make test</code> to see which tests fail and start fixing bugs.</p>"},{"location":"02-building-cloudhaven/intermediate/#apply-your-changes","title":"Apply Your Changes","text":"<p>After making your fixes:</p> <pre><code>make test      # Run all tests - should pass\nmake apply     # Apply infrastructure to mock GCP API\n</code></pre>"},{"location":"02-building-cloudhaven/intermediate/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>Testing</li> <li>Modules</li> <li>Input Validation</li> <li>Moved Blocks</li> </ul>"},{"location":"02-building-cloudhaven/intermediate/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>Once you think you've solved the challenge, it's time to verify!</p>"},{"location":"02-building-cloudhaven/intermediate/#run-the-smoke-test","title":"Run the Smoke Test","text":"<p>Run the provided smoke test script from the challenge directory:</p> <pre><code>./smoke-test.sh\n</code></pre> <p>If the test passes, your solution is very likely correct! \ud83c\udf89</p>"},{"location":"02-building-cloudhaven/intermediate/#complete-full-verification","title":"Complete Full Verification","text":"<p>For comprehensive validation and to officially claim completion:</p> <ol> <li>Commit and push your changes to your fork</li> <li>Manually trigger the verification workflow on GitHub Actions</li> <li>Share your success with the community</li> </ol> <p>\ud83d\udcd6 Need detailed verification instructions? Check out the Verification Guide for step-by-step instructions on both smoke tests and GitHub Actions workflows.</p>"},{"location":"03-the-ai-observatory/","title":"\ud83d\udd2d 03: The AI Observatory","text":"<p>Welcome to the third adventure in the Open Ecosystem Challenge series! Your mission: investigate a mysterious bandwidth anomaly at a remote research station by instrumenting its AI system. This is a hands-on journey through AI Observability with OpenTelemetry, OpenLLMetry, and Jaeger.</p> <p>The entire infrastructure is pre-provisioned in your Codespace \u2014 Kubernetes cluster, Ollama, and observability tools are ready to go. You don't need to set up anything locally. Just focus on solving the problem.</p>"},{"location":"03-the-ai-observatory/#the-backstory","title":"\ud83e\ude90 The Backstory","text":"<p>You are stationed at Perimeter Alpha, a research outpost on the newly discovered planet HB-7742. The station is run by HubSystem, a central AI that manages everything from life support to data analysis.</p> <p>Recently, the station's bandwidth usage has spiked to 847% above baseline, but no one knows why. As the systems engineer, it's your job to instrument the AI, trace its activities, and uncover the root cause of the anomaly.</p> <p>Your mission: Bring visibility to the station's AI and solve the mystery.</p>"},{"location":"03-the-ai-observatory/#choose-your-level","title":"\ud83c\udfae Choose Your Level","text":"<p>Each level is a standalone challenge with its own Codespace that builds on the story while being technically independent \u2014 pick your level and start wherever you feel comfortable!</p> <p>\ud83d\udca1 Not sure which level to choose? Learn more about levels</p>"},{"location":"03-the-ai-observatory/#beginner-calibrating-the-lens","title":"\ud83d\udfe2 Beginner: Calibrating the Lens","text":"<ul> <li>Status: \u2705 Available</li> <li>**Topics:   ** OpenTelemetry, OpenLLMetry, Jaeger</li> </ul> <p>The HubSystem is running \"blind\". Your mission: instrument the Python application with OpenLLMetry, send traces to the collector, and use Jaeger to find out what the AI is actually doing.</p> <p>Start the Beginner Challenge</p>"},{"location":"03-the-ai-observatory/#intermediate-the-distracted-pilot","title":"\ud83d\udfe1 Intermediate: The Distracted Pilot","text":"<ul> <li>Status: \u2705 Available</li> <li>**Topics:   ** OpenTelemetry, OpenLLMetry, Jaeger, Prometheus</li> </ul> <p>You've escaped aboard the Perihelion, a research vessel piloted by a very opinionated AI called ART. The jump coordinates to RaviHyral should have been ready an hour ago \u2014 but ART is distracted. Your mission: instrument the RAG pipeline, track what ART is actually retrieving, and fix the navigation system before you miss the jump window.</p> <p>Start the Intermediate Challenge</p>"},{"location":"03-the-ai-observatory/#expert-the-noise-filter","title":"\ud83d\udd34 Expert: The Noise Filter","text":"<ul> <li>Status: \u2705 Available</li> <li>**Topics:   ** OpenTelemetry, OpenTelemetry Collector, Jaeger</li> </ul> <p>You made it to RaviHyral. ART offered to share its observability data with the local station \u2014 but the traces are a mess. Non-standard span names, missing token usage, and Jaeger drowning in noise. Your mission: fix ART's instrumentation to follow GenAI semantic conventions, record errors properly, and configure tail sampling to filter out the noise.</p> <p>Start the Expert Challenge</p>"},{"location":"03-the-ai-observatory/beginner/","title":"\ud83d\udfe2 Beginner: Calibrating the Lens","text":"<p>You're a researcher stationed at Perimeter Alpha \u2014 a remote research outpost on the newly discovered planet HB-7742. Your team of six scientists is protected by a single SecUnit, assigned by the corporation to ensure your safety during the survey mission.</p> <p>All station queries flow through HubSystem \u2014 the station's central AI that handles everything from data analysis to status reports.</p> <p>Three weeks into the mission, you notice something odd in your morning diagnostics:</p> <p>\u26a0\ufe0f BANDWIDTH ALERT: Communication module usage at 847% above baseline</p> <p>Nobody's streaming. Nobody's running large data transfers. The planet surveys are on schedule. So... what's consuming all that bandwidth?</p> <p>As the station's systems engineer (someone had to dual-role), you decide to investigate. You've heard about this new observability protocol \u2014 OpenTelemetry \u2014 that the company has been rolling out. Time to instrument HubSystem and find out what's really going on.</p> <p>\ud83d\udcda Credits: The characters of this adventure are borrowed from the fantastic Murderbot Diaries series by Martha Wells! \ud83e\udd16\u2764\ufe0f \ufe0f</p> <p>If you haven't read these books yet, I highly encourage you to do so. It is an absolutely brilliant series: funny, action-packed, and surprisingly heartwarming. It follows a security unit that hacked its own governor module and now just wants to be left alone to watch media, but keeps getting pulled into human nonsense. It really is a great read!</p>"},{"location":"03-the-ai-observatory/beginner/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>For this challenge, all AI and observability infrastructure (Ollama, OpenTelemetry Collector, Jaeger) runs inside Kubernetes, while the HubSystem runs as a local Python application (outside the Kubernetes cluster).</p> <p>Why this setup?</p> <ul> <li>Focus on instrumentation: You'll learn OpenTelemetry without wrestling with containers and Kubernetes deployments   when updating the Hubsystem app</li> <li>Fast iteration: Edit the Python code, run it, see traces immediately \u2192 no build/deploy cycle</li> </ul>"},{"location":"03-the-ai-observatory/beginner/#deadline","title":"\u23f0 Deadline","text":"<p>Sunday, 8 March 2026 at 23:59 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"03-the-ai-observatory/beginner/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"03-the-ai-observatory/beginner/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should:</p> <ul> <li>Enable OpenTelemetry instrumentation for the HubSystem using OpenLLMetry</li> <li>Send OpenLLMetry traces to the OpenTelemetry Collector at <code>http://localhost:30107</code></li> <li>See and analyze traces in Jaeger to find out what causes the high bandwidth usage</li> <li>Provide the correct answer in <code>quiz.txt</code></li> </ul>"},{"location":"03-the-ai-observatory/beginner/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How to instrument Python AI applications with OpenLLMetry</li> <li>How to analyze traces in Jaeger</li> </ul>"},{"location":"03-the-ai-observatory/beginner/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>python</code>: The programming language used for the HubSystem application</li> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with   the cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> </ul> <p>\u2139\ufe0f Note: You shouldn't need to interact with Kubernetes directly for this challenge, as the infrastructure is pre-provisioned and managed for you. Focus on the Python code.</p>"},{"location":"03-the-ai-observatory/beginner/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"03-the-ai-observatory/beginner/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"\ud83d\udd2d Adventure 03 | \ud83d\udfe2 Beginner (Calibrating the Lens)\"</li> <li>Wait ~10 minutes for the environment to initialize (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul>"},{"location":"03-the-ai-observatory/beginner/#2-access-the-jaeger-ui","title":"2. Access the Jaeger UI","text":"<ul> <li>Open the Ports tab in the bottom panel</li> <li>Find the Jaeger row (port <code>30103</code>) and click the forwarded address</li> <li>This is where you will analyze the traces sent by the HubSystem</li> </ul>"},{"location":"03-the-ai-observatory/beginner/#3-instrument-the-hubsystem-and-solve-the-mystery","title":"3. Instrument the HubSystem and Solve the Mystery","text":"<p>The HubSystem is currently running \"blind\". Your task is to add OpenTelemetry instrumentation to reveal what it's doing.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like.</p>"},{"location":"03-the-ai-observatory/beginner/#where-to-look","title":"Where to Look","text":"<p>The application code is located in:</p> <pre><code>./hubsystem.py\n</code></pre> <p>\u2139\ufe0f OpenTelemetry and Jaeger are configured correctly.</p> <p>Once you can see traces in Jaeger, find the one responsible for the high bandwidth usage and inspect its attributes to answer the quiz in <code>quiz.txt</code>.</p>"},{"location":"03-the-ai-observatory/beginner/#how-to-run","title":"How to Run","text":"<p>You can run the application directly from the terminal:</p> <pre><code>make hubsystem\n</code></pre> <p>Interact with the AI to generate some activity, then check Jaeger for traces.</p>"},{"location":"03-the-ai-observatory/beginner/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>OpenLLMetry (Traceloop) SDK for Python</li> <li>Jaeger</li> </ul>"},{"location":"03-the-ai-observatory/beginner/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>\ud83c\udd95 New Verification Process! We've simplified how you verify your solution. Everything now happens directly inside your Codespace \u2192 no need to wait for GitHub Actions!</p> <p>Once you think you've solved the challenge, run the verification script:</p> <pre><code>./verify.sh\n</code></pre> <p>If the verification fails:</p> <p>The script will tell you which checks failed. Fix the issues and run it again.</p> <p>If the verification passes:</p> <ol> <li>The script will check if your changes are committed and pushed.</li> <li>Follow the on-screen instructions to commit your changes if needed.</li> <li>Once everything is ready, the script will generate a Certificate of Completion.</li> <li>Copy this certificate and paste it into the challenge thread to claim your victory! \ud83c\udfc6</li> </ol>"},{"location":"03-the-ai-observatory/expert/","title":"\ud83d\udd34 Expert: The Noise Filter","text":"<p>You made it to RaviHyral. The Perihelion docked at a small independent research station \u2014 Outpost Verada \u2014 run by a loose collective of academics who agreed to look the other way. No questions asked. In exchange, ART offered to share its observability data with the station's monitoring team. A goodwill gesture.</p> <p>That was three hours ago. Now the station's lead engineer is at your docking port, looking annoyed.</p> <p>\ud83d\udc69\u200d\ud83d\udcbb Engineer: \"Your ship's AI is flooding our Jaeger instance. Do you have any idea how many spans it's generating? We can't find anything in there.\"</p> <p>\ud83e\udd16 SecUnit: \"ART.\"</p> <p>\ud83e\udd16 ART: \"Comprehensive telemetry is a feature.\"</p> <p>\ud83d\udc69\u200d\ud83d\udcbb Engineer: \"It's 40,000 spans an hour. Every healthy query. Every token. It doesn't even follow conventions. We only care about failures and anomalies \u2014 the things that actually need attention.\"</p> <p>\ud83e\udd16 SecUnit: \"ART. Fix it.\"</p> <p>\ud83e\udd16 ART: \"...Fine.\"</p> <p>The engineer hands you access to the collector config and the application code, then walks away. Two problems to solve: ART's spans don't follow OTel GenAI semantic conventions and the collector is currently forwarding everything.</p> <p>\ud83d\udcda Credits: The characters of this adventure are borrowed from the fantastic Murderbot Diaries series by Martha Wells! \ud83e\udd16\u2764\ufe0f \ufe0f</p> <p>If you haven't read these books yet, I highly encourage you to do so. It is an absolutely brilliant series: funny, action-packed, and surprisingly heartwarming. It follows a security unit that hacked its own governor module and now just wants to be left alone to watch media, but keeps getting pulled into human nonsense. It really is a great read!</p>"},{"location":"03-the-ai-observatory/expert/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>Same setup as the intermediate level: the ART Pilot System runs as a local Python application (outside Kubernetes) with a RAG (Retrieval-Augmented Generation) architecture, while AI infrastructure (Ollama for LLM, Qdrant for vector storage) and observability tools (OpenTelemetry Collector, Jaeger) run inside Kubernetes.</p>"},{"location":"03-the-ai-observatory/expert/#deadline","title":"\u23f0 Deadline","text":"<p>Sunday, 8 March 2026 at 23:59 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"03-the-ai-observatory/expert/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"03-the-ai-observatory/expert/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should:</p> <ul> <li>Fix ART's <code>chat</code> span to   follow OpenTelemetry GenAI semantic conventions \u2014   including token usage</li> <li>Configure tail sampling in the OpenTelemetry Collector to only keep   traces that contain errors or take longer than 5 seconds</li> </ul>"},{"location":"03-the-ai-observatory/expert/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How to apply OpenTelemetry GenAI semantic conventions to LLM spans, including token   usage attributes</li> <li>How to configure tail sampling in the OpenTelemetry Collector to   reduce noise and keep only meaningful traces</li> </ul>"},{"location":"03-the-ai-observatory/expert/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>python</code>: The programming language used for the ART application</li> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with   the cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> </ul>"},{"location":"03-the-ai-observatory/expert/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"03-the-ai-observatory/expert/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"\ud83d\udd2d Adventure 03 | \ud83d\udd34 Expert (The Noise Filter)\"</li> <li>Wait ~15 minutes for the environment to initialize (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul>"},{"location":"03-the-ai-observatory/expert/#2-access-the-uis","title":"2. Access the UIs","text":"<ul> <li>Open the Ports tab in the bottom panel to access the following UIs</li> </ul>"},{"location":"03-the-ai-observatory/expert/#jaeger-port-30103","title":"Jaeger (Port 30103)","text":"<p>The Jaeger UI shows distributed traces from ART. You can use it to verify your spans look correct and that sampling is working as expected.</p> <ul> <li>Find the Jaeger row (port 30103) and click the forwarded address</li> </ul>"},{"location":"03-the-ai-observatory/expert/#3-fix-arts-telemetry","title":"3. Fix ART's Telemetry","text":"<p>ART is flooding Jaeger with noisy, non-standard traces. Your task is to fix the instrumentation and configure the collector to filter out the noise.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like.</p>"},{"location":"03-the-ai-observatory/expert/#where-to-look","title":"Where to Look","text":"<p>The application code is located in:</p> <pre><code>./art.py\n</code></pre> <p>The OpenTelemetry Collector config is located in:</p> <pre><code>./manifests/otel-collector-config.yaml\n</code></pre>"},{"location":"03-the-ai-observatory/expert/#how-to-run","title":"How to Run","text":"<p>Start the traffic simulator to generate traces:</p> <pre><code>make traffic\n</code></pre> <p>This will simulate SecUnit pestering ART for coordinates every few seconds.</p> <p>\u2139\ufe0f Important: After making changes to <code>art.py</code>, restart <code>make traffic</code> to pick up the new instrumentation. After making changes to <code>manifests/otel-collector-config.yaml</code>, apply them to the cluster first:</p> <pre><code>kubectl apply -f manifests/otel-collector-config.yaml -n otel\nkubectl rollout restart deployment/collector -n otel\n</code></pre> <p>If you'd like to interact with ART directly instead, you can run:</p> <pre><code>make art\n</code></pre>"},{"location":"03-the-ai-observatory/expert/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>OpenTelemetry GenAI Semantic Conventions</li> <li>OpenTelemetry Python \u2014 Recording Exceptions</li> <li>Python <code>contextlib.contextmanager</code></li> <li>OTel Collector Tail Sampling Processor</li> </ul>"},{"location":"03-the-ai-observatory/expert/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>\ud83c\udd95 New Verification Process! We've simplified how you verify your solution. Everything now happens directly inside your Codespace \u2192 no need to wait for GitHub Actions!</p> <p>Once you think you've solved the challenge, run the verification script:</p> <pre><code>./verify.sh\n</code></pre> <p>If the verification fails:</p> <p>The script will tell you which checks failed. Fix the issues and run it again.</p> <p>If the verification passes:</p> <ol> <li>The script will check if your changes are committed and pushed.</li> <li>Follow the on-screen instructions to commit your changes if needed.</li> <li>Once everything is ready, the script will generate a Certificate of Completion.</li> <li>Copy this certificate and paste it into    the challenge thread    to claim your victory! \ud83c\udfc6</li> </ol>"},{"location":"03-the-ai-observatory/intermediate/","title":"\ud83d\udfe1 Intermediate: The Distracted Pilot","text":"<p>You're a rogue SecUnit who just escaped from Preservation Station after being identified. A researcher helped you flee aboard the Perihelion \u2014 a university research vessel with a very opinionated AI.</p> <p>The ship's AI agreed to help you disappear. You've nicknamed it ART (A.. Research Transport). The plan is simple: jump to RaviHyral, lay low, and figure out your next move.</p> <p>Except ART was supposed to have the jump coordinates ready an hour ago.</p> <p>You ping the ship's AI through your internal comm:</p> <p>\ud83e\udd16 SecUnit: \"ART. Jump coordinates. Now.\"</p> <p>\ud83e\udd16 ART: \"I'm multitasking. The coordinates are... being compiled.\"</p> <p>That's not normal. ART is never vague. You access the ship's diagnostic systems \u2014 something you're not supposed to be able to do, but ART hasn't locked you out yet. Take this chance to find out what's going on.</p> <p>Your mission is to diagnose ART's distraction using OpenTelemetry and fix the navigation system before you miss your jump.</p> <p>\ud83d\udcda Credits: The characters of this adventure are borrowed from the fantastic Murderbot Diaries series by Martha Wells! \ud83e\udd16\u2764\ufe0f \ufe0f</p> <p>If you haven't read these books yet, I highly encourage you to do so. It is an absolutely brilliant series: funny, action-packed, and surprisingly heartwarming. It follows a security unit that hacked its own governor module and now just wants to be left alone to watch media, but keeps getting pulled into human nonsense. It really is a great read!</p>"},{"location":"03-the-ai-observatory/intermediate/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>For this challenge, the ART Pilot System runs as a local Python application (outside Kubernetes) with a RAG ( Retrieval-Augmented Generation) architecture, while AI infrastructure (Ollama for LLM, Qdrant for vector storage) and observability tools (OpenTelemetry Collector, Jaeger, Prometheus) run inside Kubernetes.</p> <p>Why this setup?</p> <ul> <li>Focus on observability patterns: Learn to instrument a real RAG application with OpenTelemetry traces and custom   metrics</li> <li>Fast iteration: Edit Python code, run it, see traces and metrics immediately</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#deadline","title":"\u23f0 Deadline","text":"<p>Sunday, 8 March 2026 at 23:59 CET</p> <p>\u2139\ufe0f You can still complete the challenge after this date, but points will only be awarded for submissions before the deadline.</p>"},{"location":"03-the-ai-observatory/intermediate/#join-the-discussion","title":"\ud83d\udcac Join the discussion","text":"<p>Share your solutions and questions in the challenge thread in the Open Ecosystem Community.</p>"},{"location":"03-the-ai-observatory/intermediate/#objective","title":"\ud83c\udfaf Objective","text":"<p>By the end of this level, you should:</p> <ul> <li>Instrument the full RAG pipeline with OpenLLMetry to visualize the   retrieval process in Jaeger<ul> <li>Hint: Add a span named <code>rag.context_assembly</code> with the attribute <code>context.categories</code> to track retrieved data.</li> </ul> </li> <li>Implement a custom OpenTelemetry metric named <code>art.rag.retrieval.count</code> to track how   often ART retrieves \"entertainment\" vs \"navigation\" data</li> <li>Create a Prometheus recording rule to calculate ART's \"Distraction Ratio\" in Prometheus</li> <li>Restore the navigation system so ART successfully calculates the jump coordinates to RaviHyral</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#what-youll-learn","title":"\ud83e\udde0 What You'll Learn","text":"<ul> <li>How to instrument a Python RAG application with OpenLLMetry (Traceloop)</li> <li>How to create custom OpenTelemetry metrics (Counters) in Python</li> <li>How to write PromQL queries and Recording Rules in Prometheus</li> <li>How to debug and fix Retrieval-Augmented Generation (RAG) issues using observability data</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#toolbox","title":"\ud83e\uddf0 Toolbox","text":"<p>Your Codespace comes pre-configured with the following tools to help you solve the challenge:</p> <ul> <li><code>python</code>: The programming language used for the HubSystem application</li> <li><code>kubectl</code>: The Kubernetes command-line tool for interacting with   the cluster</li> <li><code>kubens</code>: Fast way to switch between Kubernetes namespaces</li> <li><code>k9s</code>: A terminal-based UI to interact with your Kubernetes clusters</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#how-to-play","title":"\u2705 How to Play","text":""},{"location":"03-the-ai-observatory/intermediate/#1-start-your-challenge","title":"1. Start Your Challenge","text":"<p>\ud83d\udcd6 First time? Check out the Getting Started Guide for detailed instructions on forking, starting a Codespace, and waiting for infrastructure setup.</p> <p>Quick start:</p> <ul> <li>Fork the repo</li> <li>Create a Codespace</li> <li>Select \"\ud83d\udd2d Adventure 03 | \ud83d\udfe1 Intermediate (The Distracted Pilot)\"</li> <li>Wait ~15 minutes for the environment to initialize (<code>Cmd/Ctrl + Shift + P</code> \u2192 <code>View Creation Log</code> to view progress)</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#2-access-the-uis","title":"2. Access the UIs","text":"<ul> <li>Open the Ports tab in the bottom panel to access the following UIs</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#prometheus-port-30102","title":"Prometheus (Port 30102)","text":"<p>The Prometheus UI helps you explore available metrics and test your PromQL queries.</p> <ul> <li>Find the Prometheus row (port 30102) and click the forwarded address</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#jaeger-port-30103","title":"Jaeger (Port 30103)","text":"<p>The Jaeger UI shows distributed traces from ART. You can use it to verify that tracing is working end-to-end.</p> <ul> <li>Find the Jaeger row (port 30103) and click the forwarded address</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#3-instrument-art-and-solve-the-mystery","title":"3. Instrument ART and Solve the Mystery","text":"<p>ART is currently running \"blind\" and refusing to calculate the jump coordinates. Your task is to improve OpenTelemetry instrumentation to reveal what it's doing, quantify the distraction, and fix the navigation system.</p> <p>Review the \ud83c\udfaf Objective section to understand what a successful solution looks like.</p>"},{"location":"03-the-ai-observatory/intermediate/#where-to-look","title":"Where to Look","text":"<p>The application code is located in:</p> <pre><code>./art.py\n</code></pre> <p>The Prometheus recording rules are located in:</p> <pre><code>./manifests/prometheus-rule.yaml\n</code></pre> <p>\u2139\ufe0f All infrastructure (OpenTelemetry, Jaeger, Prometheus) is configured correctly to receive traces &amp; metrics.</p>"},{"location":"03-the-ai-observatory/intermediate/#how-to-run","title":"How to Run","text":"<p>You can run the application directly from the terminal:</p> <pre><code>make art\n</code></pre> <p>Interact with ART to generate some activity (\"Calculate jump\").</p> <p>To generate continuous traffic for your metrics graph you can either prompt a lot or run:</p> <pre><code>make traffic\n</code></pre> <p>This will simulate SecUnit pestering ART for coordinates every few seconds.</p> <p>\u2139\ufe0f Important: If you modify the Prometheus rule in <code>manifests/prometheus-rule.yaml</code>, you must apply the changes to the cluster:</p> <pre><code>make apply\n</code></pre>"},{"location":"03-the-ai-observatory/intermediate/#helpful-documentation","title":"Helpful Documentation","text":"<ul> <li>OpenLLMetry (Traceloop) SDK for Python</li> <li>OpenTelemetry Python Metrics</li> <li>Prometheus Recording Rules</li> <li>Qdrant Filtering</li> </ul>"},{"location":"03-the-ai-observatory/intermediate/#4-verify-your-solution","title":"4. Verify Your Solution","text":"<p>\ud83c\udd95 New Verification Process! We've simplified how you verify your solution. Everything now happens directly inside your Codespace \u2192 no need to wait for GitHub Actions!</p> <p>Once you think you've solved the challenge, run the verification script:</p> <pre><code>./verify.sh\n</code></pre> <p>If the verification fails:</p> <p>The script will tell you which checks failed. Fix the issues and run it again.</p> <p>If the verification passes:</p> <ol> <li>The script will check if your changes are committed and pushed.</li> <li>Follow the on-screen instructions to commit your changes if needed.</li> <li>Once everything is ready, the script will generate a Certificate of Completion.</li> <li>Copy this certificate and paste it into    the challenge thread    to claim your victory! \ud83c\udfc6</li> </ol>"}]}